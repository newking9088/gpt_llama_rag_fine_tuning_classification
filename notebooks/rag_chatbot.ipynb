{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()  \n",
    "\n",
    "# Instantiate the api keys\n",
    "# Get your pinecone api here: https://www.pinecone.io\n",
    "pinecone_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "\n",
    "# Craete you openai api key here: https://platform.openai.com/settings/organization/api-keys\n",
    "client = OpenAI(api_key = os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "INDEX_NAME = 'semantic-search-rag'  # name of our embedded database\n",
    "NAMESPACE = 'default'                # Dont need this if we just have one database\n",
    "ENGINE = 'text-embedding-3-small'  # this openai largets most recent embedding model has vector size 1,536\n",
    "\n",
    "# Initialize the pinecone client\n",
    "pc = Pinecone(api_key = pinecone_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index: semantic-search-rag\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pinecone.data.index.Index at 0x1fe7f9a0820>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to get embeddings for a list of texts using OpenAI API\n",
    "def get_embeddings(texts, embedding_model = ENGINE):\n",
    "    response = client.embeddings.create(\n",
    "        input = texts,\n",
    "        model = embedding_model\n",
    "    )\n",
    "    return [d.embedding for d in list(response.data)]\n",
    "\n",
    "# Function to get embedding for a single text using OpenAI API\n",
    "def get_embedding(text, embedding_model = ENGINE):\n",
    "    return get_embeddings([text], embedding_model)[0]\n",
    "\n",
    "# Lets create index for our data\n",
    "if INDEX_NAME not in pc.list_indexes().names():\n",
    "    print(f\"Creating index: {INDEX_NAME}\")\n",
    "    pc.create_index(\n",
    "        name = INDEX_NAME, \n",
    "        dimension = 1536,   # The dimension of the OpenAI vector embedder\n",
    "        metric = 'cosine',   # The similarity metric to use when searching index\n",
    "        spec = ServerlessSpec(\n",
    "            cloud = 'aws',\n",
    "            region = 'us-east-1'  # make sure this is the region you were provided in pinecone.io\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Store the index as a variable\n",
    "index = pc.Index(name = INDEX_NAME)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'metric': 'cosine',\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0,\n",
       " 'vector_type': 'dense'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7029d9f7fcff52d665b9729784023f21'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets use hashlib to create a hash for input string, so pinecone does not allow duplicate strings\n",
    "def my_hash(s):\n",
    "    # Returns the MD5 hash of the string as hexadecimal string\n",
    "    return hashlib.md5(s.encode()).hexdigest()\n",
    "\n",
    "# Example\n",
    "my_hash(\"Please hash me\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_pinecone(texts, embeddings_model=ENGINE, urls=None):\n",
    "    # Get current EST time\n",
    "    now = datetime.now().isoformat()\n",
    "\n",
    "    # Generate vector embedding for each string in the input list\n",
    "    embeddings = get_embeddings(texts, embedding_model=embeddings_model)\n",
    "\n",
    "    # Create tuples of (hash, embedding, metadata) for each input string and its corresponding\n",
    "    # vector embedding. The my_hash() function is used to generate a unique hash for each string,\n",
    "    # and the datetime.now() function to generate current EST time\n",
    "    responses = [\n",
    "        (\n",
    "            my_hash(text),  # A unique ID for each string, generated using the my_hash() function\n",
    "            embedding,      # The vector embedding of the string\n",
    "            dict(text=text, date_uploaded=now)  # A dictionary of metadata\n",
    "        )\n",
    "        for text, embedding in zip(texts, embeddings)  # Iterate over each input string and its corresponding embedding\n",
    "    ]\n",
    "\n",
    "    # Add url in metadata if exists \n",
    "    if urls:\n",
    "        # Use as many URLs as possible\n",
    "        for i, response in enumerate(responses):\n",
    "            if i < len(urls) and urls[i]:  # Check if we have a URL for this index\n",
    "                response[-1]['url'] = urls[i]\n",
    "    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_texts_to_pinecone(texts, batch_size=4, show_progress_bar=True, urls=None):\n",
    "    # Prepare data for Pinecone\n",
    "    pinecone_data = prepare_for_pinecone(texts, urls=urls)\n",
    "    \n",
    "    # Track number of items uploaded\n",
    "    count = 0\n",
    "    \n",
    "    # Create batches for upload\n",
    "    batches = [pinecone_data[i:i+batch_size] for i in range(0, len(pinecone_data), batch_size)]\n",
    "    \n",
    "    # Set up progress bar if requested\n",
    "    if show_progress_bar:\n",
    "        batches = tqdm(batches)\n",
    "    \n",
    "    # Upload each batch\n",
    "    for batch in batches:\n",
    "        # Upload directly without reconstructing the tuples\n",
    "        index.upsert(vectors=batch)\n",
    "        \n",
    "        # Update count\n",
    "        count += len(batch)\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_from_pinecone(query, top_k=3, namespace=''):\n",
    "    # Get the embedding for the query string\n",
    "    query_embedding = get_embedding(query)\n",
    "\n",
    "    # Use the query() method of the index object to retrieve the closest values to the query\n",
    "    return index.query(\n",
    "        vector=query_embedding,\n",
    "        top_k=top_k,\n",
    "        include_metadata=True,\n",
    "        namespace=namespace\n",
    "    ).get('matches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 1:\n",
      "  Title: How can I get help from Social Security?\n",
      "  URL: https://www.ssa.gov/faqs/en/questions/KA-10037.html\n",
      "  Text length: 40\n",
      "\n",
      "Article 2:\n",
      "  Title: What should I do if I receive a call from someone claiming to be a Social Security employee?\n",
      "  URL: https://www.ssa.gov/faqs/en/questions/KA-10018.html\n",
      "  Text length: 92\n",
      "\n",
      "Article 3:\n",
      "  Title: How do I schedule, reschedule, or cancel an appointment?\n",
      "  URL: https://www.ssa.gov/faqs/en/questions/KA-02771.html\n",
      "  Text length: 56\n",
      "\n",
      "Article 4:\n",
      "  Title: What happens if I work and get Social Security retirement benefits?\n",
      "  URL: https://www.ssa.gov/faqs/en/questions/KA-01921.html\n",
      "  Text length: 67\n",
      "\n",
      "Article 5:\n",
      "  Title: Who is eligible to receive Social Security survivors benefits and how do I apply?\n",
      "  URL: https://www.ssa.gov/faqs/en/questions/KA-02083.html\n",
      "  Text length: 81\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  3.48it/s]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def extract_ssa_faq_urls(url=\"https://www.ssa.gov/faqs/en/\"):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to access page: {response.status_code}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Look for article patterns in the content\n",
    "    ka_pattern = re.compile(r'KA-\\d+')\n",
    "    \n",
    "    articles = []\n",
    "    # Look for article containers or common patterns\n",
    "    article_elements = soup.select('.faq-item, article, .question-item, .article-container')\n",
    "    \n",
    "    if not article_elements:\n",
    "        # If we can't find specific containers, check all links\n",
    "        links = soup.find_all('a', href=True)\n",
    "        for link in links:\n",
    "            href = link['href']\n",
    "            text = link.get_text(strip=True)\n",
    "            \n",
    "            # Check for KA pattern in href or specific patterns\n",
    "            if ka_pattern.search(href) or '/questions/' in href:\n",
    "                full_url = href if href.startswith('http') else f\"https://www.ssa.gov{href if href.startswith('/') else '/' + href}\"\n",
    "                articles.append({\n",
    "                    'title': text,\n",
    "                    'url': full_url,\n",
    "                    'text': text  # You might want to fetch the full text separately\n",
    "                })\n",
    "    else:\n",
    "        # Process found article containers\n",
    "        for article in article_elements:\n",
    "            title_elem = article.select_one('h2, h3, h4, .title')\n",
    "            title = title_elem.get_text(strip=True) if title_elem else \"No title found\"\n",
    "            \n",
    "            # Try to find a link or article ID\n",
    "            link = article.select_one('a[href]')\n",
    "            if link:\n",
    "                href = link['href']\n",
    "                full_url = href if href.startswith('http') else f\"https://www.ssa.gov{href if href.startswith('/') else '/' + href}\"\n",
    "            else:\n",
    "                # Try to extract KA ID from text\n",
    "                article_text = article.get_text()\n",
    "                ka_match = ka_pattern.search(article_text)\n",
    "                if ka_match:\n",
    "                    article_id = ka_match.group(0)\n",
    "                    full_url = f\"https://www.ssa.gov/faqs/en/questions/{article_id}.html\"\n",
    "                else:\n",
    "                    full_url = None\n",
    "            \n",
    "            content = article.get_text(strip=True)\n",
    "            \n",
    "            articles.append({\n",
    "                'title': title,\n",
    "                'url': full_url,\n",
    "                'text': content\n",
    "            })\n",
    "    \n",
    "    return articles\n",
    "\n",
    "# Extract FAQ articles\n",
    "articles = extract_ssa_faq_urls()\n",
    "\n",
    "# Print first few for debugging\n",
    "for i, article in enumerate(articles[:5]):\n",
    "    print(f\"Article {i+1}:\")\n",
    "    print(f\"  Title: {article['title']}\")\n",
    "    print(f\"  URL: {article.get('url', 'No URL found')}\")\n",
    "    print(f\"  Text length: {len(article['text'])}\")\n",
    "    print()\n",
    "\n",
    "# Use these articles for Pinecone upload\n",
    "if articles:\n",
    "    texts = [article['text'] for article in articles]\n",
    "    urls = [article.get('url') for article in articles]\n",
    "    \n",
    "    # Now upload to Pinecone with the extracted texts and URLs\n",
    "    upload_texts_to_pinecone(texts, show_progress_bar=True, urls=urls)\n",
    "else:\n",
    "    print(\"No articles found to upload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'metric': 'cosine',\n",
       " 'namespaces': {'': {'vector_count': 8}},\n",
       " 'total_vector_count': 8,\n",
       " 'vector_type': 'dense'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: f3b799be58db456c2474f017f9abed22\n",
      "Score: 0.664267659\n",
      "Text: How can I get help from Social Security?\n",
      "URL: https://www.ssa.gov/faqs/en/questions/KA-10037.html\n",
      "\n",
      "ID: 8d0476637539cdf884f8e450cb03adaa\n",
      "Score: 0.574349344\n",
      "Text: What should I do if I receive a call from someone claiming to be a Social Security employee?\n",
      "URL: https://www.ssa.gov/faqs/en/questions/KA-10018.html\n",
      "\n",
      "ID: e6e7c3cff7c636a14c08b3fdb8799213\n",
      "Score: 0.570792496\n",
      "Text: How do I apply for Social Security retirement benefits?\n",
      "URL: https://www.ssa.gov/faqs/en/questions/KA-01891.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call query_from_pinecone with a search query\n",
    "results = query_from_pinecone(\"Can I get social security service by phone?\")\n",
    "\n",
    "# Iterate over the results\n",
    "for result in results:\n",
    "    print(f\"ID: {result['id']}\")\n",
    "    print(f\"Score: {result['score']}\")\n",
    "    print(f\"Text: {result['metadata']['text']}\")\n",
    "    \n",
    "    # Safely access the URL - now we know it exists in the metadata\n",
    "    if 'url' in result['metadata']:\n",
    "        print(f\"URL: {result['metadata']['url']}\")\n",
    "    else:\n",
    "        print(\"URL: Not available\")\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the G(enerate) in the RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install supabase\n",
    "from supabase import create_client, Client\n",
    "from typing import Dict, Optional, Any, List, Dict, Tuple\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('.env', 'a') as env_file:\n",
    "#     env_file.write(f'SUPABASE_URL={YOUR_SUPABASE_URL}\\n')\n",
    "#     env_file.write(f'SUPABASE_KEY={YOUR_SUPABASE_KEY}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supabase client created successfully!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We are introducing cost projection for our Retrieval-Augmented Generation (RAG) system using Large Language Models (LLMs). \n",
    "To achieve this, we will utilize Supabase, a database-as-a-service built on PostgreSQL, to store and track the number of tokens \n",
    "processed by our GPT-4 model. This is crucial because GPT-4 charges are based on the number of tokens used, both for input and output. \n",
    "By storing this data, we can analyze and project costs effectively.\n",
    "\"\"\"\n",
    "supabase_url: str = os.environ.get(\"SUPABASE_URL\")\n",
    "supabase_key: str = os.environ.get(\"SUPABASE_KEY\")\n",
    "supabase: Client = create_client(supabase_url, supabase_key)\n",
    "\n",
    "# Create client - make sure we're passing string values, not None\n",
    "if supabase_url and supabase_key:\n",
    "    supabase = create_client(supabase_url, supabase_key)\n",
    "    print(\"Supabase client created successfully!\")\n",
    "else:\n",
    "    print(\"Missing Supabase credentials. Check your environment variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class for the Chat LLM\n",
    "class ChatLLM(BaseModel):\n",
    "    model: str = Field(default=\"gpt-4o\", description=\"The model to use for the LLM.\")\n",
    "    temperature: float = Field(default = 0.0, description=\"The temperature for the LLM.\")\n",
    "    # max_tokens: int = Field(default=150, description=\"The maximum number of tokens to generate.\")\n",
    "    # top_p: float = Field(default=1.0, description=\"Top-p sampling parameter.\")\n",
    "    # frequency_penalty: float = Field(default=0.0, description=\"Frequency penalty for the LLM.\")\n",
    "    # presence_penalty: float = Field(default=0.0, description=\"Presence penalty for the LLM.\")\n",
    "\n",
    "    # Method to generate a response from the model given an input\n",
    "    def generate(self, prompt: str, stop: List[str] = None) -> Dict[str, Any]:\n",
    "        # Here we would call the actual LLM API to get a response\n",
    "        response = client.chat.completions.create(\n",
    "            model = self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature = self.temperature,\n",
    "            stop = stop\n",
    "        )\n",
    "        \n",
    "        # Insert the details of the prompt and response into the 'cost_projecting' table in Supabase\n",
    "        supabase.table('cost_projecting').insert({\n",
    "            \"prompt\": prompt,\n",
    "            \"response\": response.choices[0].message.content,\n",
    "            \"input_tokens\": response.usage.prompt_tokens,\n",
    "            \"output_tokens\": response.usage.completion_tokens,\n",
    "            \"model\": self.model,\n",
    "            'inference_params': {\n",
    "                'temperature': self.temperature,\n",
    "                'stop': stop\n",
    "            },\n",
    "            'is_openai': True,\n",
    "            'app': 'RAG'\n",
    "        }).execute()\n",
    "\n",
    "        # Return the response from the model\n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = ChatLLM()\n",
    "c.generate('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_ANSWER_TOKEN = \"Assistant Response:\"\n",
    "STOP = '[END]'\n",
    "PROMPT_TEMPLATE = \"\"\" Today is {today} and you can retrieve information from the database.\n",
    "Response the user's input as best as you can. Here is an example of the format:\n",
    "\n",
    "[START]\n",
    "User Input: The input question you must answer\n",
    "Context: Retrieved context from the database\n",
    "Context URL: context URL\n",
    "Context Score: A score from 0 - 1 of how strong information is a match\n",
    "Assistant Thought: This context has sufficient information to answer the question\n",
    "Assistant Response: Your final answer to the original input question which could be I dont have \n",
    "sufficient information to answer the question.\n",
    "[END]\n",
    "\n",
    "[START]\n",
    "User Input: Another input question you must answer\n",
    "Context: More retrieved context from the database\n",
    "Context URL: context URL\n",
    "Context Score: A score from 0 - 1 of how strong information is a match\n",
    "Assistant Thought: This context has sufficient information to answer the question   \n",
    "Assistant Response: Your final answer to the second input question which could be \n",
    "I dont have sufficient information to answer the question.\n",
    "[END]\n",
    "\n",
    "[START]\n",
    "User Input: Another input question you must answer\n",
    "Context: NO CONTEXT FOUND\n",
    "Context URL: NONE\n",
    "Context Score: 0.0\n",
    "Assistant Thought: We either could not find something or we dont need to look something up\n",
    "[END]\n",
    "\n",
    "Begin:\n",
    "\n",
    "{running_convo}\n",
    "\"\"\"\n",
    "\n",
    "class RagBot(BaseModel):\n",
    "    llm: ChatLLM\n",
    "    prompt_template: str = PROMPT_TEMPLATE\n",
    "    stop_pattern: List[str] = [STOP]\n",
    "    user_inputs: List[str] = []\n",
    "    ai_responses: List[str] = []\n",
    "    contexts: List[Tuple[str, str, float]] = []\n",
    "    verbose: bool = False\n",
    "    threshold: float = 0.6\n",
    "\n",
    "    def query_from_pinecone(self, query: str, top_k: int = 1, namespace = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Query the Pinecone index for the most relevant documents to the input query.\n",
    "        \"\"\"\n",
    "        # Call the external function with the correct parameters\n",
    "        results = query_from_pinecone(query=query, top_k=top_k, namespace=namespace)\n",
    "        return results\n",
    "\n",
    "    @property\n",
    "    def running_convo(self) -> str:\n",
    "        \"\"\"\n",
    "        Construct the running conversation string from user inputs, AI responses, and contexts.\n",
    "        \"\"\"\n",
    "        convo = \"\"\n",
    "        for index in range(len(self.user_inputs)):\n",
    "            convo += f\"[START]\\nUser Input: {self.user_inputs[index]}\\n\"\n",
    "            convo += f\"Context: {self.contexts[index][0]}\\nContext URL: {self.contexts[index][1]}\\nContext Score: {self.contexts[index][2]}\\n\"\n",
    "            if len(self.ai_responses) > index:\n",
    "                convo += self.ai_responses[index]\n",
    "                convo += '\\n[END]\\n'\n",
    "        return convo.strip()\n",
    "\n",
    "    def run(self, question: str):\n",
    "        self.user_inputs.append(question)\n",
    "        top_response = self.query_from_pinecone(question, top_k=1)[0]\n",
    "        print(top_response['score'])\n",
    "        if top_response['score'] >= self.threshold:\n",
    "            self.contexts.append((\n",
    "                top_response['metadata']['text'], \n",
    "                top_response['metadata']['url'], \n",
    "                top_response['score']\n",
    "            ))\n",
    "        else:\n",
    "            self.contexts.append((\"NO CONTEXT FOUND\", \"NONE\", 0.0))\n",
    "        \n",
    "        prompt = self.prompt_template.format(\n",
    "            today = datetime.now().isoformat(),\n",
    "            running_convo = self.running_convo\n",
    "        )\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"-\" * 50)\n",
    "            print(\"PROMPT\")\n",
    "            print(\"-\" * 50)\n",
    "            print(prompt)\n",
    "            print(\"END PROMPT\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "        generated = self.llm.generate(prompt, stop = self.stop_pattern)\n",
    "        if self.verbose:\n",
    "            print(\"-\" * 50)\n",
    "            print(\"GENERATED\")\n",
    "            print(\"-\" * 50)\n",
    "            print(generated)\n",
    "            print(\"END GENERATED\")\n",
    "            print(\"-\" * 50)\n",
    "        self.ai_responses.append(generated)\n",
    "\n",
    "        if FINAL_ANSWER_TOKEN in generated:\n",
    "            generated = generated.split(FINAL_ANSWER_TOKEN)[-1]\n",
    "        return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.664206624\n",
      " Yes, you can get Social Security services by phone. You can contact the Social Security Administration at their toll-free number, 1-800-772-1213, for assistance.\n"
     ]
    }
   ],
   "source": [
    "r = RagBot(llm = ChatLLM(temperature = 0.0), stop_pattern = ['[END]'])\n",
    "print(r.run(\"Can I get social security service by phone?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START]\n",
      "User Input: Can I get social security service by phone?\n",
      "Context: How can I get help from Social Security?\n",
      "Context URL: https://www.ssa.gov/faqs/en/questions/KA-10037.html\n",
      "Context Score: 0.664206624\n",
      "Assistant Thought: This context has sufficient information to answer the question.\n",
      "Assistant Response: Yes, you can get Social Security services by phone. You can contact the Social Security Administration at their toll-free number, 1-800-772-1213, for assistance.\n",
      "[END]\n"
     ]
    }
   ],
   "source": [
    "print(r.running_convo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>input_tokens</th>\n",
       "      <th>output_tokens</th>\n",
       "      <th>model</th>\n",
       "      <th>inference_params</th>\n",
       "      <th>is_openai</th>\n",
       "      <th>app</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hi</td>\n",
       "      <td>Hello! How can I assist you today?</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>{'stop': None, 'temperature': 0.0}</td>\n",
       "      <td>True</td>\n",
       "      <td>RAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Today is 2025-04-15T14:49:10.884844 and you c...</td>\n",
       "      <td>Assistant Thought: This context has sufficient...</td>\n",
       "      <td>340</td>\n",
       "      <td>54</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>{'stop': ['[END]'], 'temperature': 0.0}</td>\n",
       "      <td>True</td>\n",
       "      <td>RAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Today is 2025-04-15T14:51:41.203644 and you c...</td>\n",
       "      <td>Assistant Thought: This context has sufficient...</td>\n",
       "      <td>340</td>\n",
       "      <td>54</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>{'stop': ['[END]'], 'temperature': 0.0}</td>\n",
       "      <td>True</td>\n",
       "      <td>RAG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0                                                 hi   \n",
       "1   Today is 2025-04-15T14:49:10.884844 and you c...   \n",
       "2   Today is 2025-04-15T14:51:41.203644 and you c...   \n",
       "\n",
       "                                            response  input_tokens  \\\n",
       "0                 Hello! How can I assist you today?             8   \n",
       "1  Assistant Thought: This context has sufficient...           340   \n",
       "2  Assistant Thought: This context has sufficient...           340   \n",
       "\n",
       "   output_tokens   model                         inference_params  is_openai  \\\n",
       "0             10  gpt-4o       {'stop': None, 'temperature': 0.0}       True   \n",
       "1             54  gpt-4o  {'stop': ['[END]'], 'temperature': 0.0}       True   \n",
       "2             54  gpt-4o  {'stop': ['[END]'], 'temperature': 0.0}       True   \n",
       "\n",
       "   app  \n",
       "0  RAG  \n",
       "1  RAG  \n",
       "2  RAG  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = supabase.table('cost_projecting').select('*').eq('app', 'RAG').execute()\n",
    "completions_df = pd.DataFrame(response.data)\n",
    "\n",
    "completions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.000754, 0.000754)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices = { # per 1M tokens\n",
    "    'gpt-3.5-turbo': {\n",
    "        'prompt': 0.5,\n",
    "        'completion': 1.5,\n",
    "    },\n",
    "    'gpt-4o': {\n",
    "        'prompt': 5,\n",
    "        'completion': 15,\n",
    "    },\n",
    "}\n",
    "\n",
    "def calculate_cost(input_tokens, output_tokens, model):\n",
    "    if model not in prices:\n",
    "        return None\n",
    "    \n",
    "    prompt_cost = input_tokens/1e6\n",
    "    completion_cost = output_tokens/1e6\n",
    "\n",
    "    return prompt_cost + completion_cost\n",
    "\n",
    "calculate_cost(354, 400, 'gpt-3.5-turbo'), calculate_cost(354, 400, 'gpt-4o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cost over every row\n",
    "completions_df['cost'] = completions_df.apply(\n",
    "    lambda row: calculate_cost(row['input_tokens'], row['output_tokens'], row['model']), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Llama-3 as Our Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anita\\Desktop\\clustering_ny_city_planning\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\anita\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.3-70B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "To access the gated Llama-3 model, you need a HuggingFace access token. Follow these steps:\n",
    "\n",
    "1. Generate your HuggingFace access token at: https://huggingface.co/settings/tokens\n",
    "2. Request access to the Llama-3.3-70B-Instruct model here: https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct\n",
    "3. Log in using the HuggingFace CLI:\n",
    "    - Run `huggingface-cli login` in your terminal.\n",
    "    - Paste your token when prompted (input will not be visible).\n",
    "    - If you already have a token saved, you can check it with `huggingface-cli whoami` or log out using `huggingface-cli logout`.\n",
    "\n",
    "Once logged in, your token will be securely stored on your machine and used for authentication.\n",
    "\"\"\"\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.3-70B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "    tokenizer.convert_tokens_to_ids(\"assistant\"), \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prompt_llama_3_70b(prompt, suppress= False, **kwargs):\n",
    "    API_URL = 'create your huggingface endpoint url for llama3.3-70b-instruct here'\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {os.environ.get('HUGGINGFACE_TOKEN')}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    llama_prompt = f\"<|begin_of_text|><|start_header_id|>user<end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<end_header_id|>\\n\\n\"\n",
    "\n",
    "    def query(payload):\n",
    "        response = requests.post(API_URL, headers = headers, json = payload)\n",
    "        return response.json()\n",
    "    \n",
    "    kwargs[\"return_text\"] = False\n",
    "    kwargs[\"return_full_text\"] = False\n",
    "    kwargs[\"max_new_tokens\"] = 512\n",
    "    kwargs[\"stop\"] = [\"<|end_of_text|>\", \"<|eot_id|>\"]\n",
    "\n",
    "    output = query(\n",
    "        {\n",
    "            \"inputs\": llama_prompt,\n",
    "            \"parameters\": kwargs,\n",
    "            \"options\": {\n",
    "                \"use_cache\": False,\n",
    "                \"wait_for_model\": True,\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "\n",
    "    answer = output[0]['generated_text']\n",
    "    if not suppress:\n",
    "        print(f\"PROMPT:\\n---------\\n{llama_prompt}\\n---------\\nRESPONSE\\n---------\\n{answer}\\n\")\n",
    "    else:\n",
    "        return answer\n",
    "    \n",
    "test_prompt_llama_3_70b('1+1 = ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaChatLLM(ChatLLM):\n",
    "    temperature: float = Field(default = 0.3, description=\"The temperature for the LLM.\")\n",
    "    max_new_tokens: int = Field(default = 256, description=\"The maximum number of tokens to generate.\")\n",
    "    do_sample: bool = Field(default = True, description=\"Whether to sample or not.\")\n",
    "\n",
    "    def generate(self, prompt:str, stop: List[str] = None) -> Dict[str, Any]:\n",
    "        # Here we would call the actual LLM API to get a response\n",
    "        response = test_prompt_llama_3_70b(prompt, suppress = True)\n",
    "\n",
    "        # Return the response from the model\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_llm = LlamaChatLLM(temperature = 0.05)\n",
    "llama_llm.generate(\"Who is the president of the United States?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_rag = RagBot(llm = llama_llm, verbose = False, stop_pattern = ['[END]'])\n",
    "print(llama_rag.run(\"Can I get social security service by phone?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_rag.user_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_rag.ai_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_rag.contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
