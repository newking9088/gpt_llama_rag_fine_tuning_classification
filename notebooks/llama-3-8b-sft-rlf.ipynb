{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Building a Chat-Aligned LLM Using Quantized LoRA Fine-Tuning\n\n## Introduction\n\nThis note explains how to take an open-source Large Language Model (LLM) like Llama-3 and transform it into a chat-aligned assistant through memory-efficient fine-tuning. We'll use Parameter-Efficient Fine-Tuning (PEFT) with Low-Rank Adaptation (LoRA) to achieve this with minimal computational resources.\n\n## What Is Quantized LoRA Fine-Tuning?\n\nFine-tuning combines two powerful techniques:\n\n1. **Quantization**: Reducing model precision (e.g., from 32-bit to 4-bit) to decrease memory usage\n2. **LoRA (Low-Rank Adaptation)**: Training small adapter matrices instead of the entire model\n\nTogether, these techniques enable efficient fine-tuning of billion-parameter models on accessible hardware.\n\n\n## Understanding LoRA\n\nLoRA works by inserting small trainable matrices into the attention layers:\n\n1. **Original Operation**: `Y = WX` where `W` is a large weight matrix\n2. **LoRA Modification**: `Y = WX + ΔWX` where `ΔW = BA` (low-rank decomposition)\n3. **Benefits**: `B` and `A` are much smaller than `W`, reducing trainable parameters\n\n## Why Is This Useful?\n\n- **Resource Efficiency**: Train 8B+ parameter models on a single consumer GPU or TPU\n- **Parameter Efficiency**: Update only ~0.1% of model parameters instead of 100%\n- **Knowledge Preservation**: Retain the base model's knowledge while adding new capabilities\n- **Format Adaptation**: Convert \"raw\" models into chat-optimized assistants\n\n## Prerequisites\n\n- Access to TPU or other computing environment\n- Hugging Face account with API token\n\n## Step-by-Step Implementation\n\n### 1. Environment Setup\n\nFirst, install the necessary libraries:\n\n```python\n!pip install transformers peft datasets torch_xla torch\n```\n\n### 2. Authentication with Hugging Face\n\n```python\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_KEY\")\nlogin(token=hf_token)\n```\n\n### 3. Define Special Tokens for Chat Format\n\nThe key to chat alignment is implementing a specific format with special tokens:\n\n```python\nSPECIAL_TOKENS = {\n    'stop_token': {\n        'token': '###STOP###',\n        'replace_embedding_with': 'stop_talking'\n    },\n    'human_token': {\n        'token': '###HUMAN###',\n        'replace_embedding_with': 'human_speaking'\n    },\n    'bot_token': {\n        'token': '###BOT###',\n        'replace_embedding_with': 'assistant_speaking'\n    }\n}\n```\n\nThese tokens help the model distinguish between human input and assistant responses.\n\n### 4. Load and Prepare the Base Model\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Load tokenizer and add special tokens\ntokenizer = AutoTokenizer.from_pretrained(\n    \"meta-llama/Meta-Llama-3.1-8B\",  \n    trust_remote_code=True\n)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.add_tokens([info['token'] for info in SPECIAL_TOKENS.values()])\n\n# Load model with TPU optimizations\nimport torch_xla.core.xla_model as xm\ndevice = xm.xla_device()\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Meta-Llama-3.1-8B\",\n    torch_dtype=torch.bfloat16,\n    low_cpu_mem_usage=True\n)\nmodel = model.to(device)\n\n# Resize embeddings to accommodate new tokens\nmodel.resize_token_embeddings(len(tokenizer))\n```\n\n### 5. Initialize Special Token Embeddings\n\n```python\n# Initialize embeddings with semantically similar words\nfor token_name, token_info in SPECIAL_TOKENS.items():\n    token_id = tokenizer(token_info['token'])['input_ids'][-1]\n    similar_word_ids = tokenizer(token_info['replace_embedding_with'])['input_ids'][1:]\n    new_embedding = model.model.embed_tokens.weight.data[similar_word_ids].cpu().mean(dim=0, keepdim=True)\n    model.model.embed_tokens.weight.data[token_id] = new_embedding.to(model.device).clone()\n```\n\n### 6. Configure LoRA for Efficient Training\n\n```python\nfrom peft import LoraConfig, get_peft_model\n\n# Create LoRA configuration\nlora_config = LoraConfig(\n    r=8,                        # Rank of LoRA matrices\n    lora_alpha=16,              # Scaling factor\n    target_modules=[\"q_proj\", \"v_proj\"],  # Attention layers to modify\n    lora_dropout=0.1,           # Dropout for regularization\n    bias=\"none\",                # Don't train bias terms\n    task_type=\"CAUSAL_LM\"       # Causal language modeling task\n)\n\n# Apply LoRA to the model\nmodel = get_peft_model(model, lora_config)\n\n# Print statistics\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Trainable parameters: {trainable_params:,} ({100 * trainable_params/total_params:.3f}% of total)\")\n```\n\n### 7. Prepare Conversation Dataset\n\n```python\nfrom datasets import Dataset, load_dataset\n\n# Load conversation datasets\nassistant_dataset = load_dataset(\"timdettmers/openassistant-guanaco\")\noriginal_guanaco_dataset = load_dataset(\"guanaco/guanaco\")\n\n# Parse conversations and format with special tokens\n# (See full implementation in the code for details)\n\n# Create and tokenize dataset\ntokenized_dataset = {\n    \"train\": train_dataset.map(tokenize_function, batched=True),\n    \"test\": test_dataset.map(tokenize_function, batched=True)\n}\n```\n\n### 8. Configure Training\n\n```python\nfrom transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./llama-lora-finetuned\",\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=8,\n    learning_rate=1e-4,\n    num_train_epochs=1,\n    bf16=True,  # Use bfloat16 on TPUs\n    label_names=[\"labels\"]\n)\n\n# Data collator\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False  # Not using masked language modeling\n)\n\n# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    data_collator=data_collator,\n    label_names=[\"labels\"]\n)\n```\n\n### 9. Train the Model\n\n```python\n# Start training\nprint(\"Starting training...\")\ntrainer.train()\n\n# Save the fine-tuned model\nmodel.save_pretrained(\"./llama-lora-finetuned\")\ntokenizer.save_pretrained(\"./llama-lora-finetuned\")\n```\n\n### 10. Test the Fine-Tuned Model\n\n```python\n# Test with a simple prompt\ntest_prompt = \"What is machine learning?\"\ninputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n\noutputs = model.generate(\n    **inputs,\n    max_new_tokens=50,\n    do_sample=True,\n    temperature=0.7\n)\n\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(f\"Generated: {generated_text}\")\n```\n\n### 11. Upload to Hugging Face (Optional)\n\n```python\n# Upload to Hugging Face Hub\ndef upload_to_huggingface(model, tokenizer, output_dir, repository_id, token, private=True):\n    model.push_to_hub(\n        repository_id,\n        use_auth_token=token,\n        private=private\n    )\n    \n    tokenizer.push_to_hub(\n        repository_id,\n        use_auth_token=token,\n        private=private\n    )\n    \n    print(f\"Model uploaded to https://huggingface.co/{repository_id}\")\n\n# Example usage\nrepository_id = \"your-username/llama-3-chat-lora\"\nupload_to_huggingface(model, tokenizer, \"./llama-lora-finetuned\", repository_id, hf_token)\n```\n\n### Conversation Format\n\nThe chat-aligned model uses special tokens to structure conversations:\n\n```\n###HUMAN###What is machine learning?###BOT###Machine learning is a subfield of artificial intelligence...###STOP###\n```\n\nThis format teaches the model when to start and stop generating responses.\n\n## Using Your Fine-Tuned Model\n\nTo use your newly chat-aligned model:\n\n```python\nfrom peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Load the fine-tuned model\nconfig = PeftConfig.from_pretrained(\"your-username/llama-3-chat-lora\")\nmodel = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\nmodel = PeftModel.from_pretrained(model, \"your-username/llama-3-chat-lora\")\ntokenizer = AutoTokenizer.from_pretrained(\"your-username/llama-3-chat-lora\")\n\n# Function to format user inputs for the model\ndef format_prompt(user_input):\n    return f\"###HUMAN###{user_input}###BOT###\"\n\n# Generate a response\ndef generate_response(user_input, max_length=100):\n    prompt = format_prompt(user_input)\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    \n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=max_length,\n        do_sample=True,\n        temperature=0.7,\n        top_p=0.9\n    )\n    \n    # Extract just the assistant's response\n    full_response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n    assistant_response = full_response.split(\"###BOT###\")[-1].split(\"###STOP###\")[0]\n    \n    return assistant_response.strip()\n\n# Example usage\nresponse = generate_response(\"Explain quantum computing in simple terms\")\nprint(response)\n```\n\n## References\n\n- [Hugging Face PEFT Documentation](https://huggingface.co/docs/peft/index)\n- [LoRA Paper](https://arxiv.org/abs/2106.09685)\n- [QLoRA Paper](https://arxiv.org/abs/2305.14314)","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade pip\n!pip install --no-warn-script-location transformers accelerate peft bitsandbytes bitsandbytes>=0.43.0 datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T15:33:52.250640Z","iopub.execute_input":"2025-04-27T15:33:52.250982Z","iopub.status.idle":"2025-04-27T15:33:59.202485Z","shell.execute_reply.started":"2025-04-27T15:33:52.250954Z","shell.execute_reply":"2025-04-27T15:33:59.201679Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\nCollecting pip\n  Downloading pip-25.1-py3-none-any.whl.metadata (3.6 kB)\nDownloading pip-25.1-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 24.1.2\n    Uninstalling pip-24.1.2:\n      Successfully uninstalled pip-24.1.2\nSuccessfully installed pip-25.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\"\"\"\nTPU-Compatible LoRA Fine-tuning for Llama-3 Models with Checkpointing\n\nThis script implements efficient fine-tuning of Llama-3 language models using:\n1. Low-Rank Adaptation (LoRA) for parameter-efficient training\n2. Special token handling for conversation formatting\n3. TPU compatibility for Kaggle TPU environments\n4. Checkpointing for resumable training\n\nAuthor: Nawaraj Paudel, PhD\nDate: April 26, 2025\n\"\"\"\n\nimport os\nimport time\nimport numpy as np\nimport torch\nimport torch_xla\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nfrom datasets import Dataset, load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling\n)\nimport pandas as pd\nfrom peft import LoraConfig, get_peft_model, PeftModel\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nfrom tqdm.auto import tqdm\nimport logging\nimport datetime\n\n# Set up logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger(\"LoRA-Finetune\")\n\n# Kaggle-specific paths\nKAGGLE_WORKING_DIR = \"/kaggle/working\"\nOUTPUT_DIR = os.path.join(KAGGLE_WORKING_DIR, \"llama-lora-finetuned\")\n\n\ndef setup_authentication():\n    \"\"\"\n    Set up authentication with Hugging Face using Kaggle secrets.\n    \n    Returns:\n        str: Authentication token\n    \"\"\"\n    logger.info(\"Setting up Hugging Face authentication...\")\n    \n    # Access the Hugging Face token from Kaggle secrets\n    user_secrets = UserSecretsClient()\n    secret_value_0 = user_secrets.get_secret(\"HF_KEY\")\n    \n    # Log in to Hugging Face Hub\n    login(token=secret_value_0)\n    logger.info(\"Successfully authenticated with Hugging Face\")\n    \n    return secret_value_0\n\n\ndef load_tokenizer(model_name, special_tokens=None):\n    \"\"\"\n    Load the tokenizer and add special tokens if provided.\n    \n    Args:\n        model_name (str): Name or path of the pretrained model\n        special_tokens (dict, optional): Dictionary of special tokens to add\n    \n    Returns:\n        tokenizer: The loaded tokenizer with special tokens\n    \"\"\"\n    logger.info(f\"Loading tokenizer from {model_name}...\")\n    \n    # Load the tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_name, \n        trust_remote_code=True\n    )\n    \n    # Set padding token and padding side\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.padding_side = 'left'  # Left padding for causal language modeling\n    \n    # Add special tokens if provided\n    if special_tokens:\n        tokens_to_add = [info['token'] for info in special_tokens.values()]\n        num_added = tokenizer.add_tokens(tokens_to_add)\n        logger.info(f\"Added {num_added} special tokens to the tokenizer\")\n    \n    return tokenizer\n\n\ndef load_tpu_model(model_name, token=None):\n    \"\"\"\n    Load a pretrained model for TPU training.\n    \n    Args:\n        model_name (str): Name or path of the pretrained model\n        token (str, optional): Authentication token\n    \n    Returns:\n        model: The loaded model\n    \"\"\"\n    logger.info(f\"Loading model {model_name} for TPU training...\")\n    start_time = time.time()\n    \n    # Get TPU device\n    device = xm.xla_device()\n    \n    # Load the model with TPU compatibility settings\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        torch_dtype=torch.bfloat16,  # TPUs work well with bfloat16\n        low_cpu_mem_usage=True,      # Reduce memory usage during loading\n        token=token                  # Authentication token\n    )\n    \n    # Move model to TPU\n    model = model.to(device)\n    \n    elapsed_time = time.time() - start_time\n    logger.info(f\"Successfully loaded model on TPU (took {elapsed_time:.2f} seconds)\")\n    return model\n\n\ndef initialize_special_token_embeddings(model, tokenizer, special_tokens):\n    \"\"\"\n    Initialize embeddings for special tokens using semantically similar words.\n    \n    Args:\n        model: The loaded model\n        tokenizer: The tokenizer with special tokens\n        special_tokens (dict): Dictionary of special tokens and their embeddings\n    \n    Returns:\n        dict: Updated special_tokens dictionary with token IDs and embeddings\n    \"\"\"\n    logger.info(\"Initializing special token embeddings...\")\n    \n    for token_name, token_info in special_tokens.items():\n        token_id = tokenizer(token_info['token'])['input_ids'][-1]\n        similar_word_ids = tokenizer(token_info['replace_embedding_with'])['input_ids'][1:]\n        \n        # Move to CPU for embedding manipulation\n        new_embedding = model.model.embed_tokens.weight.data[similar_word_ids].cpu().mean(dim=0, keepdim=True)\n        \n        special_tokens[token_name]['new_embedding'] = new_embedding\n        \n        # Move embedding to TPU before updating\n        model.model.embed_tokens.weight.data[token_id] = new_embedding.to(model.device).clone()\n        special_tokens[token_name]['token_id'] = token_id\n        \n        logger.info(f\"Initialized embedding for {token_info['token']} (ID: {token_id})\")\n    \n    return special_tokens\n\n\ndef create_lora_config(rank=8, alpha=16, dropout=0.1):\n    \"\"\"\n    Create a TPU-compatible LoRA configuration.\n    \n    Args:\n        rank (int): Rank of the low-rank matrices\n        alpha (int): Scaling factor for the low-rank updates\n        dropout (float): Dropout probability for regularization\n    \n    Returns:\n        LoraConfig: TPU-compatible configuration for LoRA training\n    \"\"\"\n    logger.info(f\"Creating LoRA configuration (rank={rank}, alpha={alpha}, dropout={dropout})...\")\n    \n    # Create LoRA configuration optimized for TPU\n    return LoraConfig(\n        r=rank,\n        lora_alpha=alpha,\n        target_modules=[\"q_proj\", \"v_proj\"],  # Target key layers\n        lora_dropout=dropout,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\"\n    )\n\n\ndef apply_lora_adapters(model, lora_config):\n    \"\"\"\n    Apply LoRA adapters to the model for parameter-efficient fine-tuning.\n    \n    Args:\n        model: The loaded model\n        lora_config: LoRA configuration\n    \n    Returns:\n        model: Model with LoRA adapters\n    \"\"\"\n    logger.info(\"Applying LoRA adapters to the model...\")\n    \n    # Apply LoRA adapters to the model\n    model = get_peft_model(model, lora_config)\n    \n    # Print trainable parameter statistics\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    total_params = sum(p.numel() for p in model.parameters())\n    logger.info(f\"Trainable parameters: {trainable_params:,} ({100 * trainable_params/total_params:.3f}% of total)\")\n    \n    return model\n\n\ndef parse_conversations(json_data):\n    \"\"\"\n    Parse conversation data from different dataset formats.\n    \n    Args:\n        json_data (dict): Data entry from dataset\n        \n    Returns:\n        list: List of (speaker, message) tuples representing the conversation\n    \"\"\"\n    if 'prompt' in json_data and 'response' in json_data:\n        return [['human', json_data['prompt']], ['assistant', json_data['response']]]\n    \n    conversations = json_data['text'].split('###')\n    \n    convos = []\n    for convo in conversations:\n        if convo.strip():\n            parts = convo.strip().split(': ', 1)\n            if len(parts) == 2:\n                speaker, message = parts\n                convos.append((speaker.lower(), message))\n    \n    return convos\n\n\ndef load_and_merge_datasets(tokenizer, max_length=256, test_size=0.2, seed=42, max_samples=2000):\n    \"\"\"\n    Load, parse, and merge multiple conversation datasets with TPU optimizations.\n    \n    Args:\n        tokenizer: Tokenizer for processing the text\n        max_length (int): Maximum sequence length\n        test_size (float): Proportion of data to use for testing\n        seed (int): Random seed for reproducibility\n        max_samples (int): Maximum number of samples to use\n    \n    Returns:\n        dataset_dict: Dictionary containing 'train' and 'test' datasets\n    \"\"\"\n    logger.info(f\"Loading and merging datasets (max_samples={max_samples})...\")\n    start_time = time.time()\n    \n    # Load the datasets with streaming\n    logger.info(\"Loading openassistant-guanaco dataset...\")\n    assistant_dataset = load_dataset(\"timdettmers/openassistant-guanaco\", streaming=True)\n    \n    logger.info(\"Loading guanaco dataset...\")\n    original_guanaco_dataset = load_dataset(\"guanaco/guanaco\", streaming=True)\n    \n    # Convert streaming datasets to lists with limited samples\n    logger.info(\"Processing openassistant examples...\")\n    assistant_data = []\n    for i, example in tqdm(enumerate(assistant_dataset['train']), \n                           desc=\"Loading openassistant examples\", \n                           total=max_samples // 2):\n        if i >= max_samples // 2:\n            break\n        assistant_data.append({'conversation': parse_conversations(example)})\n    \n    logger.info(\"Processing guanaco examples...\")\n    guanaco_data = []\n    for i, example in tqdm(enumerate(original_guanaco_dataset['train']), \n                           desc=\"Loading guanaco examples\", \n                           total=max_samples // 2):\n        if i >= max_samples // 2:\n            break\n        guanaco_data.append({'conversation': parse_conversations(example)})\n    \n    logger.info(f\"Loaded {len(assistant_data)} assistant examples and {len(guanaco_data)} guanaco examples\")\n    \n    # Combine the datasets\n    combined_data = assistant_data + guanaco_data\n    \n    # Shuffle the data\n    import random\n    random.seed(seed)\n    random.shuffle(combined_data)\n    \n    # Create train/test split\n    split_idx = int(len(combined_data) * (1 - test_size))\n    train_data = combined_data[:split_idx]\n    test_data = combined_data[split_idx:]\n    \n    logger.info(f\"Split into {len(train_data)} training examples and {len(test_data)} test examples\")\n    \n    # Filter to keep only valid conversations\n    logger.info(\"Filtering conversations...\")\n    train_data = [ex for ex in train_data if \n                  ex['conversation'] and \n                  len(ex['conversation']) % 2 == 0 and \n                  ex['conversation'][-1][0] == 'assistant']\n    \n    test_data = [ex for ex in test_data if \n                ex['conversation'] and \n                len(ex['conversation']) % 2 == 0 and \n                ex['conversation'][-1][0] == 'assistant']\n    \n    logger.info(f\"Filtered to {len(train_data)} training examples and {len(test_data)} test examples\")\n    \n    # Format conversations with special tokens\n    def join_conversation(example):\n        convo_text = \"\"\n        last_speaker = None\n        \n        for speaker, message in example['conversation']:\n            last_speaker = speaker\n            if speaker == 'human':\n                convo_text += f\"###HUMAN###{message}\"\n            elif speaker == 'assistant':\n                convo_text += f\"###BOT###{message}\"\n        \n        if last_speaker == 'human':\n            convo_text = convo_text.strip() + \"###BOT###\"\n        else:\n            convo_text = convo_text.strip() + \"###STOP###\"\n        \n        return {\"text\": convo_text}\n    \n    logger.info(\"Formatting conversations with special tokens...\")\n    train_formatted = [join_conversation(ex) for ex in tqdm(train_data, desc=\"Formatting train data\")]\n    test_formatted = [join_conversation(ex) for ex in tqdm(test_data, desc=\"Formatting test data\")]\n    \n    # Create datasets\n    train_dataset = Dataset.from_dict({\"text\": [ex['text'] for ex in train_formatted]})\n    test_dataset = Dataset.from_dict({\"text\": [ex['text'] for ex in test_formatted]})\n    \n    # Tokenize the datasets in batches\n    def tokenize_function(examples):\n        return tokenizer(\n            examples[\"text\"],\n            padding=\"max_length\",\n            truncation=True,\n            max_length=max_length\n        )\n    \n    # Apply tokenization\n    logger.info(f\"Tokenizing datasets (max_length={max_length})...\")\n    train_tokenized = train_dataset.map(\n        tokenize_function, \n        batched=True, \n        batch_size=32,\n        desc=\"Tokenizing training data\"\n    )\n    test_tokenized = test_dataset.map(\n        tokenize_function, \n        batched=True, \n        batch_size=32,\n        desc=\"Tokenizing test data\"\n    )\n    \n    # Create a combined dataset dictionary\n    tokenized_dataset = {\n        \"train\": train_tokenized,\n        \"test\": test_tokenized\n    }\n    \n    elapsed_time = time.time() - start_time\n    logger.info(f\"Dataset preparation completed in {elapsed_time:.2f} seconds\")\n    \n    return tokenized_dataset\n\n\ndef create_training_args(output_dir, batch_size=4, gradient_accumulation_steps=8, \n                         learning_rate=1e-4, num_epochs=1):\n    \"\"\"\n    Create TPU-compatible training arguments with checkpointing.\n    \n    Args:\n        output_dir (str): Directory to save model checkpoints\n        batch_size (int): Batch size for TPU\n        gradient_accumulation_steps (int): Steps to accumulate gradients\n        learning_rate (float): Learning rate\n        num_epochs (float): Number of training epochs\n    \n    Returns:\n        TrainingArguments: TPU-compatible training arguments\n    \"\"\"\n    logger.info(f\"Creating training arguments (batch_size={batch_size}, lr={learning_rate}, epochs={num_epochs})...\")\n    \n    return TrainingArguments(\n        output_dir=output_dir,\n        per_device_train_batch_size=batch_size,\n        gradient_accumulation_steps=gradient_accumulation_steps,\n        learning_rate=learning_rate,\n        num_train_epochs=num_epochs,\n        # Enable checkpoint saving\n        save_strategy=\"steps\",       # Save at regular steps\n        save_steps=200,              # Save every 200 steps\n        save_total_limit=3,          # Keep only the 3 most recent checkpoints\n        # Other parameters\n        logging_dir=f\"{output_dir}/logs\",\n        logging_strategy=\"steps\",\n        logging_steps=10,\n        report_to=\"none\",\n        # TPU-specific settings\n        bf16=True,  # Use bfloat16 precision (native on TPUs)\n        label_names=[\"labels\"],  # Set label names explicitly\n        # We don't use TensorFlow Profiler\n        xla_tpu_config={\n            \"iterations_per_loop\": 100,\n        }\n    )\n\n\ndef setup_trainer(model, training_args, tokenized_dataset, tokenizer):\n    \"\"\"\n    Set up the trainer with TPU optimizations.\n    \n    Args:\n        model: The model to train\n        training_args: Training arguments\n        tokenized_dataset: Tokenized dataset\n        tokenizer: Tokenizer\n    \n    Returns:\n        Trainer: Configured trainer\n    \"\"\"\n    logger.info(\"Setting up trainer...\")\n    \n    # Create a data collator\n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer,\n        mlm=False\n    )\n    \n    # Calculate estimated training time\n    num_examples = len(tokenized_dataset[\"train\"])\n    steps_per_epoch = num_examples // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)\n    total_steps = steps_per_epoch * training_args.num_train_epochs\n    # Estimated time: ~3 seconds per step for TPU (rough estimate)\n    estimated_time_seconds = total_steps * 3\n    estimated_time = str(datetime.timedelta(seconds=int(estimated_time_seconds)))\n    \n    logger.info(f\"Training will run for approximately {total_steps} steps\")\n    logger.info(f\"Estimated training time: {estimated_time}\")\n    \n    # Create the trainer with XLA compilation\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset[\"train\"],\n        eval_dataset=tokenized_dataset[\"test\"],\n        data_collator=data_collator,\n        label_names=[\"labels\"]\n    )\n    \n    # Add custom logging callback\n    class LoggingCallback(TrainerCallback):\n        def __init__(self, trainer):\n            self.trainer = trainer\n            self.start_time = time.time()\n        \n        def on_step_end(self, args, state, control, **kwargs):\n            if state.global_step % args.logging_steps == 0:\n                elapsed = time.time() - self.start_time\n                steps_per_second = state.global_step / elapsed\n                remaining_steps = state.max_steps - state.global_step\n                remaining_time = remaining_steps / steps_per_second\n                remaining_time_str = str(datetime.timedelta(seconds=int(remaining_time)))\n                \n                progress = state.global_step / state.max_steps * 100\n                logger.info(f\"Progress: {progress:.2f}% - Step: {state.global_step}/{state.max_steps}\")\n                logger.info(f\"Loss: {state.log_history[-1]['loss']:.4f}\")\n                logger.info(f\"Estimated time remaining: {remaining_time_str}\")\n    \n    from transformers.trainer_callback import TrainerCallback\n    trainer.add_callback(LoggingCallback(trainer))\n    \n    return trainer\n\n\ndef test_model(model, tokenizer, test_prompt):\n    \"\"\"\n    Test the fine-tuned model with a simple prompt.\n    \n    Args:\n        model: The fine-tuned model\n        tokenizer: The tokenizer\n        test_prompt (str): Prompt to test\n    \"\"\"\n    logger.info(f\"Testing model with prompt: \\\"{test_prompt}\\\"\")\n    \n    # Tokenize the prompt\n    inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n    \n    # Generate text\n    logger.info(\"Generating response...\")\n    start_time = time.time()\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=50,\n            do_sample=True,\n            temperature=0.7,\n        )\n    \n    elapsed_time = time.time() - start_time\n    \n    # Decode the generated text\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    logger.info(f\"Generation completed in {elapsed_time:.2f} seconds\")\n    logger.info(f\"Generated response: {generated_text}\")\n    \n    return generated_text\n\n\ndef find_latest_checkpoint(output_dir):\n    \"\"\"\n    Find the latest checkpoint in the output directory.\n    \n    Args:\n        output_dir (str): Directory to search for checkpoints\n        \n    Returns:\n        str or None: Path to the latest checkpoint, or None if no checkpoints found\n    \"\"\"\n    if not os.path.exists(output_dir):\n        return None\n    \n    checkpoints = [\n        os.path.join(output_dir, d) for d in os.listdir(output_dir)\n        if os.path.isdir(os.path.join(output_dir, d)) and \"checkpoint\" in d\n    ]\n    \n    if not checkpoints:\n        return None\n    \n    # Sort checkpoints by the step number (extracted from the directory name)\n    checkpoints = sorted(\n        checkpoints,\n        key=lambda x: int(x.split(\"-\")[-1]) if x.split(\"-\")[-1].isdigit() else 0\n    )\n    \n    logger.info(f\"Found checkpoint: {checkpoints[-1]}\")\n    return checkpoints[-1]\n\n\ndef save_notebook_state():\n    \"\"\"Save the current notebook state in Kaggle\"\"\"\n    try:\n        from IPython import get_ipython\n        ipython = get_ipython()\n        if ipython is not None:\n            logger.info(\"Saving notebook state...\")\n            ipython.magic(\"notebook -e\")\n            logger.info(\"Notebook state saved\")\n    except:\n        logger.warning(\"Could not save notebook state\")\n\n\ndef main():\n    \"\"\"\n    Main function with TPU optimizations and checkpointing support.\n    \"\"\"\n    logger.info(\"=\"*50)\n    logger.info(\"Starting LoRA fine-tuning process\")\n    logger.info(\"=\"*50)\n    \n    # Create output directory if it doesn't exist\n    if not os.path.exists(OUTPUT_DIR):\n        os.makedirs(OUTPUT_DIR)\n        logger.info(f\"Created output directory: {OUTPUT_DIR}\")\n    \n    # Define model and special tokens\n    model_name = \"meta-llama/Meta-Llama-3.1-8B\"\n    \n    # Define special tokens\n    SPECIAL_TOKENS = {\n        'stop_token': {\n            'token': '###STOP###',\n            'replace_embedding_with': 'stop_talking'\n        },\n        'human_token': {\n            'token': '###HUMAN###',\n            'replace_embedding_with': 'human_speaking'\n        },\n        'bot_token': {\n            'token': '###BOT###',\n            'replace_embedding_with': 'assistant_speaking'\n        }\n    }\n    \n    # Check for existing checkpoints\n    resume_from_checkpoint = find_latest_checkpoint(OUTPUT_DIR)\n    has_existing_model = os.path.exists(os.path.join(OUTPUT_DIR, \"adapter_config.json\"))\n    \n    if resume_from_checkpoint:\n        logger.info(f\"Will resume training from checkpoint: {resume_from_checkpoint}\")\n    elif has_existing_model:\n        logger.info(f\"Found existing model at {OUTPUT_DIR}, will continue training\")\n    else:\n        logger.info(\"No checkpoint found, starting new training run\")\n    \n    # Initialize TPU\n    logger.info(\"Initializing TPU...\")\n    device = xm.xla_device()\n    logger.info(f\"Using device: {device}\")\n    \n    # Set up authentication\n    hf_token = setup_authentication()\n    \n    # Load tokenizer with special tokens\n    tokenizer = load_tokenizer(model_name, SPECIAL_TOKENS)\n    \n    # Load model based on checkpoint status\n    if resume_from_checkpoint or has_existing_model:\n        logger.info(\"Loading base model...\")\n        # First load the base model\n        base_model = load_tpu_model(model_name, hf_token)\n        base_model.resize_token_embeddings(len(tokenizer))\n        \n        # Then load LoRA adapters\n        if resume_from_checkpoint:\n            logger.info(f\"Loading LoRA adapters from checkpoint: {resume_from_checkpoint}\")\n            model = PeftModel.from_pretrained(base_model, resume_from_checkpoint)\n        else:\n            logger.info(f\"Loading LoRA adapters from: {OUTPUT_DIR}\")\n            model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n            \n        logger.info(\"Successfully loaded model with adapters\")\n    else:\n        # Start fresh training\n        logger.info(\"Starting fresh training run\")\n        \n        # Load model for TPU\n        model = load_tpu_model(model_name, hf_token)\n        \n        # Resize token embeddings and initialize special tokens\n        model.resize_token_embeddings(len(tokenizer))\n        SPECIAL_TOKENS = initialize_special_token_embeddings(model, tokenizer, SPECIAL_TOKENS)\n        \n        # Create and apply LoRA configuration\n        lora_config = create_lora_config()\n        model = apply_lora_adapters(model, lora_config)\n    \n    # Load and merge datasets\n    tokenized_dataset = load_and_merge_datasets(tokenizer)\n    \n    # Create TPU-optimized training arguments\n    training_args = create_training_args(OUTPUT_DIR)\n    \n    # Set up trainer\n    trainer = setup_trainer(model, training_args, tokenized_dataset, tokenizer)\n    \n    # Train the model\n    logger.info(\"=\"*50)\n    logger.info(\"Starting training process\")\n    logger.info(\"=\"*50)\n    \n    try:\n        # Start or resume training\n        trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n        logger.info(\"Training completed successfully!\")\n        \n        # Save notebook state\n        save_notebook_state()\n    except KeyboardInterrupt:\n        logger.info(\"Training interrupted by user\")\n        # Save model and state even when interrupted\n        logger.info(\"Saving current model state...\")\n        model.save_pretrained(OUTPUT_DIR)\n        tokenizer.save_pretrained(OUTPUT_DIR)\n        save_notebook_state()\n        logger.info(\"Current state saved\")\n        return\n    except Exception as e:\n        logger.error(f\"Training error: {str(e)}\")\n        # Try to save model state on error\n        try:\n            logger.info(\"Attempting to save current model state...\")\n            model.save_pretrained(OUTPUT_DIR)\n            tokenizer.save_pretrained(OUTPUT_DIR)\n            save_notebook_state()\n            logger.info(\"Current state saved despite error\")\n        except:\n            logger.error(\"Failed to save model state\")\n        \n        logger.info(\"Training interrupted. You can resume from the latest checkpoint.\")\n        return\n    \n    # Save the fine-tuned model\n    logger.info(\"Saving final model...\")\n    model.save_pretrained(OUTPUT_DIR)\n    tokenizer.save_pretrained(OUTPUT_DIR)\n    logger.info(f\"Model saved to {OUTPUT_DIR}\")\n    \n    # Test the model\n    logger.info(\"Testing the fine-tuned model...\")\n    test_prompt = \"What is machine learning?\"\n    test_model(model, tokenizer, test_prompt)\n    \n    logger.info(\"=\"*50)\n    logger.info(\"LoRA fine-tuning process completed\")\n    logger.info(\"=\"*50)\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Exploring individual parts of the above code","metadata":{}},{"cell_type":"markdown","source":"# Set Up\nRequest llama-3-8b gated model for your use here: https://huggingface.co/meta-llama/Meta-Llama-3-8B. You can access your HuggingFace token here: https://huggingface.co/settings/tokens. On the top menu in Kaggle, click on Add-ons > Secrets and provide key name and value there which will generate the code snippet to copy into clipboard.","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"HF_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T02:41:32.855683Z","iopub.execute_input":"2025-04-26T02:41:32.856039Z","iopub.status.idle":"2025-04-26T02:41:32.991832Z","shell.execute_reply.started":"2025-04-26T02:41:32.856017Z","shell.execute_reply":"2025-04-26T02:41:32.991276Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from huggingface_hub import login\n\n# Using the environment variable you set earlier\nlogin(token = secret_value_0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T02:41:35.590925Z","iopub.execute_input":"2025-04-26T02:41:35.591197Z","iopub.status.idle":"2025-04-26T02:41:35.695962Z","shell.execute_reply.started":"2025-04-26T02:41:35.591176Z","shell.execute_reply":"2025-04-26T02:41:35.695339Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from datasets import load_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T23:48:48.798924Z","iopub.execute_input":"2025-04-25T23:48:48.799129Z","iopub.status.idle":"2025-04-25T23:48:48.802555Z","shell.execute_reply.started":"2025-04-25T23:48:48.799114Z","shell.execute_reply":"2025-04-25T23:48:48.801839Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# lets merge these two conversational datasets together\nassistant_dataset = load_dataset(\"timdettmers/openassistant-guanaco\")\noriginal_guanaco_dataset = load_dataset(\"guanaco/guanaco\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T23:48:48.803341Z","iopub.execute_input":"2025-04-25T23:48:48.803647Z","iopub.status.idle":"2025-04-25T23:48:53.855330Z","shell.execute_reply.started":"2025-04-25T23:48:48.803631Z","shell.execute_reply":"2025-04-25T23:48:53.854637Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/395 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a23992cbadaa46f6a5d66aa42a1be05a"}},"metadata":{}},{"name":"stderr","text":"Repo card metadata block was not found. Setting CardData to empty.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"openassistant_best_replies_train.jsonl:   0%|          | 0.00/20.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c51fd91063d549bf86c8ac03fb0fc6d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"openassistant_best_replies_eval.jsonl:   0%|          | 0.00/1.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a042f3f037bd47799a1fd6e86df7f8cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/9846 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ef963971082405083688f78c760f1a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/518 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"952b148408f54435b00c153cc075e97b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/42.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e1b0c295ee84038901158698b2d840d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"guanaco.json:   0%|          | 0.00/56.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69f8ae9eb7e34844bf52203d0637ca01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/48701 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"641c61901d734a36857f376102330cd1"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"assistant_dataset['train'][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T23:48:53.856094Z","iopub.execute_input":"2025-04-25T23:48:53.856281Z","iopub.status.idle":"2025-04-25T23:48:53.860938Z","shell.execute_reply.started":"2025-04-25T23:48:53.856266Z","shell.execute_reply":"2025-04-25T23:48:53.860350Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'text': '### Human: Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.### Assistant: \"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions.\\n\\nRecent research has identified potential monopsonies in industries such as retail and fast food, where a few large companies control a significant portion of the market (Bivens & Mishel, 2013). In these industries, workers often face low wages, limited benefits, and reduced bargaining power, leading to a situation where they are dependent on the employer for their livelihood. This dependence can result in further suppression of wages and a decline in working conditions.\\n\\nOverall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue.\\n\\nReferences:\\nBivens, J., & Mishel, L. (2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.### Human: Now explain it to a dog'}"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"original_guanaco_dataset['train'][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T23:48:53.861602Z","iopub.execute_input":"2025-04-25T23:48:53.861873Z","iopub.status.idle":"2025-04-25T23:48:53.877579Z","shell.execute_reply.started":"2025-04-25T23:48:53.861856Z","shell.execute_reply":"2025-04-25T23:48:53.877087Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"{'text': \"Describe the following:\\nA tall, thin man with dark hair styled in a slick pompadour.### Response:\\nThe man is quite tall with a slender physique. His dark hair is styled in a sleek, classic pompadour with a slight sheen to it, suggesting perhaps he spends time grooming it. The hair appears to be well-maintained, as though he's put some effort into it. The dark color of his hair contrasts with his pale skin, which gives him a somewhat striking appearance. He conveys a sense of poise and confidence, as though he's comfortable in his own skin. Overall, he seems to exude a sophisticated, polished vibe.\",\n 'prompt': 'Describe the following:\\nA tall, thin man with dark hair styled in a slick pompadour.',\n 'response': \"The man is quite tall with a slender physique. His dark hair is styled in a sleek, classic pompadour with a slight sheen to it, suggesting perhaps he spends time grooming it. The hair appears to be well-maintained, as though he's put some effort into it. The dark color of his hair contrasts with his pale skin, which gives him a somewhat striking appearance. He conveys a sense of poise and confidence, as though he's comfortable in his own skin. Overall, he seems to exude a sophisticated, polished vibe.\"}"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"# Parse Conversation\n\nLets standardize each conversation into the form:\n```\n[\n('human', 'human utterance 1'),\n('assistant', 'assistant utterance 1'),\n('human', 'human utterance 2'),\n('assistant', 'assistant utterance 2')\n]\n```","metadata":{}},{"cell_type":"code","source":"# Function to parse the JSON and extract human/bot conversations\ndef parse_conversations(json_data):\n    if 'prompt' in json_data and 'response' in json_data:\n        return [['human', json_data['prompt']], ['assistant', json_data['response']]]\n    # Split the text on '###' to separate conversations\n    conversations = json_data['text'].split('###')\n\n    # Process each conversation and split each data into human and bot parts\n    convos = []\n    for convo in conversations:\n        if convo.strip():  # checks if the string is not just whitespace\n            parts = convo.strip().split(': ', 1) # Split on the first occurrence of ': '\n            if len(parts) == 2:\n                speaker,message = parts\n                convos.append((speaker.lower(), message))\n    return convos\n\n# Parse the conversations\nprint(parse_conversations(assistant_dataset['train'][0]))\nprint(parse_conversations(original_guanaco_dataset['train'][0]))\n            ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T23:48:53.878211Z","iopub.execute_input":"2025-04-25T23:48:53.878481Z","iopub.status.idle":"2025-04-25T23:48:53.894293Z","shell.execute_reply.started":"2025-04-25T23:48:53.878466Z","shell.execute_reply":"2025-04-25T23:48:53.893695Z"}},"outputs":[{"name":"stdout","text":"[('human', 'Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.'), ('assistant', '\"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions.\\n\\nRecent research has identified potential monopsonies in industries such as retail and fast food, where a few large companies control a significant portion of the market (Bivens & Mishel, 2013). In these industries, workers often face low wages, limited benefits, and reduced bargaining power, leading to a situation where they are dependent on the employer for their livelihood. This dependence can result in further suppression of wages and a decline in working conditions.\\n\\nOverall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue.\\n\\nReferences:\\nBivens, J., & Mishel, L. (2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.'), ('human', 'Now explain it to a dog')]\n[['human', 'Describe the following:\\nA tall, thin man with dark hair styled in a slick pompadour.'], ['assistant', \"The man is quite tall with a slender physique. His dark hair is styled in a sleek, classic pompadour with a slight sheen to it, suggesting perhaps he spends time grooming it. The hair appears to be well-maintained, as though he's put some effort into it. The dark color of his hair contrasts with his pale skin, which gives him a somewhat striking appearance. He conveys a sense of poise and confidence, as though he's comfortable in his own skin. Overall, he seems to exude a sophisticated, polished vibe.\"]]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"assistant_dataset = assistant_dataset.map(lambda x: {'conversation': parse_conversations(x)})\nassistant_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T23:48:53.894882Z","iopub.execute_input":"2025-04-25T23:48:53.895041Z","iopub.status.idle":"2025-04-25T23:48:54.459032Z","shell.execute_reply.started":"2025-04-25T23:48:53.895028Z","shell.execute_reply":"2025-04-25T23:48:54.458328Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9846 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bbadffa1ddc4c5f80beb9047f3170e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/518 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c316088b5394fbfae14ee24fcefa4d6"}},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'conversation'],\n        num_rows: 9846\n    })\n    test: Dataset({\n        features: ['text', 'conversation'],\n        num_rows: 518\n    })\n})"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"original_guanaco_dataset = original_guanaco_dataset.map(lambda x: {'conversation': parse_conversations(x)})\noriginal_guanaco_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T23:48:54.459737Z","iopub.execute_input":"2025-04-25T23:48:54.460319Z","iopub.status.idle":"2025-04-25T23:48:57.531715Z","shell.execute_reply.started":"2025-04-25T23:48:54.460300Z","shell.execute_reply":"2025-04-25T23:48:57.530836Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/48701 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b0088548d27486bbe9729e15203dd7c"}},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'prompt', 'response', 'conversation'],\n        num_rows: 48701\n    })\n})"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# combine the datasets together\ndataset = Dataset.from_dict({'conversation': list(assistant_dataset['train']['conversation']) + list(original_guanaco_dataset['train']['conversation'])})\ndataset = dataset.train_test_split(test_size = 0.2, seed = 42)\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T23:48:57.532487Z","iopub.execute_input":"2025-04-25T23:48:57.532715Z","iopub.status.idle":"2025-04-25T23:48:59.106570Z","shell.execute_reply.started":"2025-04-25T23:48:57.532698Z","shell.execute_reply":"2025-04-25T23:48:59.105961Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['conversation'],\n        num_rows: 46837\n    })\n    test: Dataset({\n        features: ['conversation'],\n        num_rows: 11710\n    })\n})"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# Let's define some special tokens! We aren't using Meta's standard tokens because the no-chat aligned version doesn't have any\nSTOP_TOKEN = '###STOP###'  # can be any token, we are just using STOP\nHUMAN_TOKEN = '###HUMAN###'\nBOT_TOKEN = '###BOT###'\n\n# Define the extra tokens dictionary with all three tokens\nEXTRA_TOKENS = {\n    'stop_token': {\n        'token': STOP_TOKEN,\n        'replace_embedding_with': 'stop_talking'\n    },\n    'human_token': {\n        'token': HUMAN_TOKEN,\n        'replace_embedding_with': 'human_speaking'\n    },\n    'bot_token': {\n        'token': BOT_TOKEN,\n        'replace_embedding_with': 'assistant_speaking'\n    }\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T23:48:59.107343Z","iopub.execute_input":"2025-04-25T23:48:59.107608Z","iopub.status.idle":"2025-04-25T23:48:59.111663Z","shell.execute_reply.started":"2025-04-25T23:48:59.107584Z","shell.execute_reply":"2025-04-25T23:48:59.111086Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Use model from Hugging Face - we are using non chat for mat to illustrate we dont need to start from the chat checkpoint\n# the chat checkpoint\nbase_model = 'meta-llama/Meta-Llama-3.1-8B'\n\n# Load LlaMA tokenizer and add the tree special tokens\ntokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code = True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = 'left'  # Technically not necessary\n\ntokenizer.add_tokens([extra['token'] for extra in EXTRA_TOKENS.values()])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T23:48:59.114627Z","iopub.execute_input":"2025-04-25T23:48:59.114915Z","iopub.status.idle":"2025-04-25T23:49:01.876657Z","shell.execute_reply.started":"2025-04-25T23:48:59.114898Z","shell.execute_reply":"2025-04-25T23:49:01.876010Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5fa53e3b07e4297aafb88a65e510a46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbe8290a6b644ea0a4f24c69771b5aa3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c2bdbf3aae649a7a2915ee88d903ff3"}},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"3"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# Load with PyTorch's native half precision\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    torch_dtype=torch.bfloat16,\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_compute_dtype=torch.bfloat16,\n        bnb_4bit_quant=False,\n        bnb_4bit_type='nf4',\n    ),\n)\n\npeft_model = PeftModel.from_pretrained(\n    base_model,\n    subfolder=\"lofta_init\",\n    is_trainable=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T02:18:17.207146Z","iopub.execute_input":"2025-04-26T02:18:17.207397Z","iopub.status.idle":"2025-04-26T02:18:17.274259Z","shell.execute_reply.started":"2025-04-26T02:18:17.207378Z","shell.execute_reply":"2025-04-26T02:18:17.273556Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/654714388.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load with PyTorch's native half precision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     quantization_config = BitsAndBytesConfig(\n","\u001b[0;31mNameError\u001b[0m: name 'AutoModelForCausalLM' is not defined"],"ename":"NameError","evalue":"name 'AutoModelForCausalLM' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"print(model.config.vocab_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T23:50:21.527600Z","iopub.status.idle":"2025-04-25T23:50:21.527989Z","shell.execute_reply.started":"2025-04-25T23:50:21.527795Z","shell.execute_reply":"2025-04-25T23:50:21.527813Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# resize model's embedding size to match tokenizer + 3 new special tokens\nmodel.resize_token_embeddings(len(tokenizer))\nprint(model.config.vocab_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T23:50:21.528894Z","iopub.status.idle":"2025-04-25T23:50:21.529150Z","shell.execute_reply.started":"2025-04-25T23:50:21.529030Z","shell.execute_reply":"2025-04-25T23:50:21.529044Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Custom Token Embedding Initialization\n# ------------------------------------\n# When adding new special tokens to a pre-trained model, we need to initialize their embeddings.\n# Since we're using LoRA (which cannot modify the embedding layer), we need a different approach.\nfor extra_token, extra_info in EXTRA_TOKENS.items():\n    token_id = tokenizer(extra_info['token'])['input_ids'][-1]\n    new_embedding = model.model.embed_tokens.weight.data[tokenizer(extra_info['replace_embedding_with'])['input_ids'][1:]].mean(dim=0, keepdim=True)\n    EXTRA_TOKENS[extra_token]['new_embedding'] = new_embedding\n    model.model.embed_tokens.weight.data[token_id] = EXTRA_TOKENS[extra_token]['new_embedding'].clone()\n    EXTRA_TOKENS[extra_token]['token_id'] = token_id\n    print(f\"Replaced token \\\"{extra_info['token']}\\\" (token_id {token_id}) weight with weight for \\\"{extra_info['replace_embedding_with']}\\\"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T23:50:21.530238Z","iopub.status.idle":"2025-04-25T23:50:21.530549Z","shell.execute_reply.started":"2025-04-25T23:50:21.530389Z","shell.execute_reply":"2025-04-25T23:50:21.530403Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Lets take a look into our data\ntext_pd = pd.concat([pd.DataFrame(dataset['train']), pd.DataFrame(dataset['test'])])\ntext_pd['split'] = ['train'] * len(dataset['train']) + ['test'] * len(dataset['test'])\ntext_pd['last_speaker'] = text_pd['conversation'].apply(lambda x: x[-1][0])\ntext_pd['convo_length'] = text_pd['conversation'].apply(len)\ntext_pd = text_pd[text_pd['last_speaker'].isin(['assistant', 'human'])]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T23:50:21.531676Z","iopub.status.idle":"2025-04-25T23:50:21.532015Z","shell.execute_reply.started":"2025-04-25T23:50:21.531855Z","shell.execute_reply":"2025-04-25T23:50:21.531870Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# We want conversations where assistants speeks last, those with human last arent as useful\ntext_pd['last_speaker'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T23:50:21.532635Z","iopub.status.idle":"2025-04-25T23:50:21.532926Z","shell.execute_reply.started":"2025-04-25T23:50:21.532807Z","shell.execute_reply":"2025-04-25T23:50:21.532822Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# There are multiple languages in the dataset\ntext_pd.head(2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T23:50:21.533454Z","iopub.status.idle":"2025-04-25T23:50:21.533723Z","shell.execute_reply.started":"2025-04-25T23:50:21.533610Z","shell.execute_reply":"2025-04-25T23:50:21.533622Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Only keep convo ending with assistant\ntext_pd = text_pd[text_pd['last_speaker'] == 'assistant']\ntext_pd = text_pd[text_pd['convo_length'] % 2 == 0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T23:50:21.535182Z","iopub.status.idle":"2025-04-25T23:50:21.535484Z","shell.execute_reply.started":"2025-04-25T23:50:21.535317Z","shell.execute_reply":"2025-04-25T23:50:21.535333Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text_pd = text_pd.reset_index()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T23:50:21.536673Z","iopub.status.idle":"2025-04-25T23:50:21.536954Z","shell.execute_reply.started":"2025-04-25T23:50:21.536830Z","shell.execute_reply":"2025-04-25T23:50:21.536844Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text_pd['convo_length'].value_counts().sort_index().plot.bar()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T23:50:21.537957Z","iopub.status.idle":"2025-04-25T23:50:21.538231Z","shell.execute_reply.started":"2025-04-25T23:50:21.538114Z","shell.execute_reply":"2025-04-25T23:50:21.538127Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Filter out by language","metadata":{}},{"cell_type":"code","source":"FILTER_EN = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T23:50:21.539283Z","iopub.status.idle":"2025-04-25T23:50:21.539485Z","shell.execute_reply.started":"2025-04-25T23:50:21.539383Z","shell.execute_reply":"2025-04-25T23:50:21.539391Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import pipeline\nimport pandas as pd\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T23:50:21.541180Z","iopub.status.idle":"2025-04-25T23:50:21.541432Z","shell.execute_reply.started":"2025-04-25T23:50:21.541313Z","shell.execute_reply":"2025-04-25T23:50:21.541326Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define a function for batch processing\ndef detect_language_in_batches(batch, batch_size = 128):\n    batch_results = pipe([b[0][1] for b in batch['conversation']])\n    return {'lang': [b['label'] for b in batch_results]}\n\n# Apply the function to DataFrame\nif FILTER_EN:\n    pipe = pipeline(\"text-classification\", model = \"papluca/xlm-roberta-base-language-detection\", truncation = True, max_length = 64)\n    dataset = dataset.map(detect_language_in_batches, batch_size = 128, batched = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T23:50:21.543035Z","iopub.status.idle":"2025-04-25T23:50:21.543354Z","shell.execute_reply.started":"2025-04-25T23:50:21.543231Z","shell.execute_reply":"2025-04-25T23:50:21.543247Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Set a professional style\nsns.set_style(\"whitegrid\")\nplt.rcParams['font.family'] = 'sans-serif'\n\n# Only create the language chart if we've filtered for language\nif FILTER_EN:\n    # Get language distribution\n    language_counts = pd.Series(dataset['train']['lang'].value_counts())\n    \n    # Create the visualization\n    plt.figure(figsize=(10, 6))\n    ax = language_counts.plot.bar(\n        color=sns.color_palette(\"viridis\", len(language_counts)),\n        edgecolor='black',\n        width=0.7\n    )\n    \n    # Enhance with labels and styling\n    plt.title('Distribution of Languages in Training Dataset', fontsize=16, pad=20)\n    plt.xlabel('Language', fontsize=12)\n    plt.ylabel('Number of Examples', fontsize=12)\n    plt.xticks(rotation=45, ha='right')\n    \n    # Add value labels on top of bars\n    for i, v in enumerate(language_counts):\n        ax.text(\n            i, \n            v + (language_counts.max() * 0.02),  # Slight offset above bar\n            f'{v:,}',  # Format with commas for thousands\n            ha='center',\n            fontsize=10\n        )\n    \n    # Adjust layout and display\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T23:50:21.544115Z","iopub.status.idle":"2025-04-25T23:50:21.544447Z","shell.execute_reply.started":"2025-04-25T23:50:21.544275Z","shell.execute_reply":"2025-04-25T23:50:21.544308Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if FILTER_EN:\n    dataset = dataset.filter(lambda x: x['lang'] == 'en')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T23:50:21.545510Z","iopub.status.idle":"2025-04-25T23:50:21.545714Z","shell.execute_reply.started":"2025-04-25T23:50:21.545618Z","shell.execute_reply":"2025-04-25T23:50:21.545626Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def join_convo(conversation):\n    convo = ''''''\n    last_speaker = None\n    for speaker, message in conversation:\n        last_speaker = speaker\n        if speaker == 'human':\n            convo += f\"{EXTRA_TOKENS['human_token']['token']}{message}\"\n        elif speaker == 'assistant':\n            convo += f\"{EXTRA_TOKENS['bot_token']['token']}{message}\"\n        \n    if last_speaker == 'human':\n        return convo.strip() + f\"{EXTRA_TOKENS['bot_token']['token']}\"\n    return convo.strip() + f\"{EXTRA_TOKENS['stop_token']['token']}\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T23:50:21.547198Z","iopub.status.idle":"2025-04-25T23:50:21.547502Z","shell.execute_reply.started":"2025-04-25T23:50:21.547348Z","shell.execute_reply":"2025-04-25T23:50:21.547362Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(join_convo([['human', 'who was the first president of the USA?']]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T23:50:21.549009Z","iopub.status.idle":"2025-04-25T23:50:21.549330Z","shell.execute_reply.started":"2025-04-25T23:50:21.549172Z","shell.execute_reply":"2025-04-25T23:50:21.549186Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # First, install the correct version of bitsandbytes\n# !pip install -q bitsandbytes>=0.39.0\n# !pip install -q accelerate\n\n# # Import necessary libraries\n# import numpy as np\n# import os\n# import torch\n# from datasets import Dataset, load_dataset\n# from transformers import (\n#     AutoModelForCausalLM,\n#     AutoTokenizer,\n#     BitsAndBytesConfig,\n#     TrainingArguments,\n#     Trainer,\n#     pipeline,\n#     logging,\n#     DataCollatorForLanguageModeling\n# )\n# import json\n# import pandas as pd\n# from peft import LoraConfig, PeftModel, get_peft_model\n# from tqdm import tqdm\n\n# # Set up base model identifier\n# base_model = 'meta-llama/Meta-Llama-3.1-8B'\n\n# # Define special tokens for chat format\n# STOP_TOKEN = '###STOP###'\n# HUMAN_TOKEN = '###HUMAN###'\n# BOT_TOKEN = '###BOT###'\n\n# # Define the extra tokens dictionary\n# EXTRA_TOKENS = {\n#     'stop_token': {\n#         'token': STOP_TOKEN,\n#         'replace_embedding_with': 'stop_talking'\n#     },\n#     'human_token': {\n#         'token': HUMAN_TOKEN,\n#         'replace_embedding_with': 'human_speaking'\n#     },\n#     'bot_token': {\n#         'token': BOT_TOKEN,\n#         'replace_embedding_with': 'assistant_speaking'\n#     }\n# }\n\n# # Load tokenizer and add special tokens\n# tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n# tokenizer.pad_token = tokenizer.eos_token\n# tokenizer.padding_side = 'left'\n# tokenizer.add_tokens([extra['token'] for extra in EXTRA_TOKENS.values()])\n\n# # Check if bitsandbytes is properly installed\n# try:\n#     import bitsandbytes as bnb\n#     print(f\"bitsandbytes version: {bnb.__version__}\")\n# except ImportError:\n#     print(\"bitsandbytes is not installed properly. Trying a different approach...\")\n#     !pip install -q bitsandbytes --force-reinstall\n#     import bitsandbytes as bnb\n#     print(f\"bitsandbytes version after reinstall: {bnb.__version__}\")\n\n# # Now try to configure quantization\n# try:\n#     # Configure quantization settings\n#     quantization_config = BitsAndBytesConfig(\n#         load_in_4bit=True,\n#         bnb_4bit_compute_dtype=torch.bfloat16,\n#         bnb_4bit_use_double_quant=True,\n#         bnb_4bit_quant_type='nf4',\n#     )\n    \n#     # Load model with proper quantization\n#     model = AutoModelForCausalLM.from_pretrained(\n#         base_model,\n#         quantization_config=quantization_config,\n#         torch_dtype=torch.bfloat16,\n#         device_map=\"auto\",\n#     )\n    \n#     print(\"Model loaded successfully with quantization!\")\n# except Exception as e:\n#     print(f\"Error when loading with 4-bit quantization: {e}\")\n#     print(\"Falling back to 8-bit quantization...\")\n    \n#     try:\n#         # Try 8-bit quantization as fallback\n#         quantization_config = BitsAndBytesConfig(\n#             load_in_8bit=True,\n#         )\n        \n#         model = AutoModelForCausalLM.from_pretrained(\n#             base_model,\n#             quantization_config=quantization_config,\n#             torch_dtype=torch.float16,\n#             device_map=\"auto\",\n#         )\n#         print(\"Model loaded successfully with 8-bit quantization!\")\n#     except Exception as e:\n#         print(f\"Error when loading with 8-bit quantization: {e}\")\n#         print(\"Attempting to load with float16 without quantization (warning: high memory usage)...\")\n        \n#         # Last resort - load with fp16 but no quantization\n#         model = AutoModelForCausalLM.from_pretrained(\n#             base_model,\n#             torch_dtype=torch.float16,\n#             device_map=\"auto\",\n#         )\n#         print(\"Model loaded in float16 without quantization\")\n\n# # Resize token embeddings to account for the new tokens\n# model.resize_token_embeddings(len(tokenizer))\n\n# # Initialize special token embeddings with semantically similar words\n# for extra_token, extra_info in EXTRA_TOKENS.items():\n#     token_id = tokenizer(extra_info['token'])['input_ids'][-1]\n#     replace_token_ids = tokenizer(extra_info['replace_embedding_with'])['input_ids'][1:]\n#     new_embedding = model.model.embed_tokens.weight.data[replace_token_ids].mean(dim=0, keepdim=True)\n#     EXTRA_TOKENS[extra_token]['new_embedding'] = new_embedding\n#     model.model.embed_tokens.weight.data[token_id] = EXTRA_TOKENS[extra_token]['new_embedding'].clone()\n#     print(f\"Replaced token \\\"{extra_info['token']}\\\" (token_id {token_id}) weight with weight for \\\"{extra_info['replace_embedding_with']}\\\"\")\n\n# # Configure LoRA for efficient fine-tuning\n# lora_config = LoraConfig(\n#     r=16,  # Rank\n#     lora_alpha=32,  # Alpha parameter\n#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Target the attention modules\n#     lora_dropout=0.05,\n#     bias=\"none\",\n#     task_type=\"CAUSAL_LM\",\n# )\n\n# # Apply LoRA to the base model\n# model = get_peft_model(model, lora_config)\n# print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T02:53:58.638220Z","iopub.execute_input":"2025-04-27T02:53:58.638453Z","iopub.status.idle":"2025-04-27T02:53:58.656474Z","shell.execute_reply.started":"2025-04-27T02:53:58.638427Z","shell.execute_reply":"2025-04-27T02:53:58.651330Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}