{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised Text Quality Evaluation\n",
        "\n",
        "Here we will demonstrate the following metrics to compare generated text to a refence:\n",
        "\n",
        "* BLEU (Bilingual Evaluation Understudy)\n",
        "* BERTScore\n",
        "* ROUGE (Recall-Oriented Understudy for Gisting Evaluation)"
      ],
      "metadata": {
        "id": "IbR8dZsrwPDq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install required packages"
      ],
      "metadata": {
        "id": "bVb1ILKTwsqo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "f8db7aec-9446-46ef-9ab3-b11a63a50a7a",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "C7NohBEXjTmM"
      },
      "outputs": [],
      "source": [
        "!pip install nltk rouge bert_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "nwnYtqXwwvwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bert_score import BERTScorer\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import pandas as pd\n",
        "from rouge import Rouge"
      ],
      "metadata": {
        "id": "5mpFccAAuLeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example Text\n",
        "\n",
        "We need some examples of text to score. Here we define one to use as our ground truth and two that we will compare against it:\n",
        "\n",
        "- response_a: The target; a clear and concise explanation.\n",
        "- response_b: A confusing and convoluted explanation that mixes several ideas.\n",
        "- response_c: An explanation that doesn't connect well and is very unclear."
      ],
      "metadata": {
        "id": "_LKU_ATXuwft"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "6d36357c-040c-4686-b52b-6d5ecf5e9a2f",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "4phl74qhrOiM"
      },
      "outputs": [],
      "source": [
        "response_a = \"\"\"Blockchain is a decentralized digital ledger that records and\n",
        "stores data in a secure and transparent manner. It is a chain of blocks, where\n",
        "each block contains a set of transactions that are verified and added to the\n",
        "chain through a consensus mechanism. This means that the data stored on the\n",
        "blockchain cannot be altered or deleted, making it immutable and tamper-proof.\n",
        "The data is also distributed across a network of computers, making it highly\n",
        "secure and resistant to hacking. This technology is most commonly associated\n",
        "with cryptocurrencies, but it has many other potential applications such as\n",
        "supply chain management, voting systems, and smart contracts. Overall,\n",
        "blockchain provides a reliable and efficient way to store and transfer data\n",
        "without the need for intermediaries, making it a revolutionary technology with\n",
        "the potential to transform various industries.\"\"\"\n",
        "\n",
        "response_b = \"\"\"Blockchain is internet technology which distributes a duplicate\n",
        "record to all nodes in order to protect the network from fraud or dishonesty.\n",
        "The digital ledger holds transactions and allows instant verification by the\n",
        "entire system. All copies or blocks of data connected one after another into\n",
        "processing lines form blockchain. It was first developed for bitcoin payments\n",
        "but can be used for other cryptocurrency in major organizations.Finally,\n",
        "blockchain has a large scalability and stores data in a block structure that is\n",
        "permanently chained together. Blockchain tech is on internet where same record\n",
        "gets spread on all nodes concurrently keeping net secure from fraudulence.\n",
        "Ledger keeps track of transactions through instantaneous confirmation with the\n",
        "whole setup.\"\"\"\n",
        "\n",
        "response_c = \"\"\"Think of the blockchain as a Jenga tower that is constantly\n",
        "growing and adding new blocks at the top, while also reinforcing and connecting\n",
        "each block to the blocks underneath it. This tower only moves forward with\n",
        "consensus from all the players involved, but anyone can contribute their own\n",
        "block as long as they follow the predetermined rules and maintain transparency\n",
        "within the chain. This ensures an ever-evolving and decentralized network that\n",
        "maintains immutability through shared verification processes. Essentially, it's\n",
        "like creating a never-ending game of three-dimensional collaborative digital\n",
        "Tetris while wearing blindfolds made of pseudo-random algorithms.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute Evaluation Metrics"
      ],
      "metadata": {
        "id": "i0r1p0c8xG2z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BLEU\n",
        "\n",
        "- **Interpretation**: Higher BLEU scores indicate closer word choice and phrasing to the reference, suggesting higher **coherence** and **precision** in the generated response. However, BLEU is sensitive to exact matches, which can sometimes penalize well-phrased responses that use different wordings.\n",
        "- **Typical Range**: BLEU scores range from 0 to 1 (or 0–100%), with higher scores representing greater overlap with the reference. Scores above 0.5 (50%) often indicate high similarity, but even a lower score can reflect fluency if the response is phrased differently."
      ],
      "metadata": {
        "id": "_cHwaq3DxTM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_bleu(reference, hypothesis):\n",
        " reference = reference.split()  # Tokenize the reference\n",
        " hypothesis = hypothesis.split()  # Tokenize the hypothesis\n",
        " return sentence_bleu([reference], hypothesis)\n",
        "\n",
        "bleu_b = compute_bleu(response_a, response_b)\n",
        "bleu_c = compute_bleu(response_a, response_c)"
      ],
      "metadata": {
        "id": "QOPJ20y7xMQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"BLEU Score for Response B: {bleu_b:.3f}\")\n",
        "print(f\"BLEU Score for Response C: {bleu_c:.3f}\")"
      ],
      "metadata": {
        "id": "WN49ceGBxRb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERTScore\n",
        "\n",
        "- **Interpretation**: A high BERTScore indicates that the generated response is semantically close to the reference, which implies both **coherence** (in terms of aligned ideas) and **fluency** (capturing meaning even with varied wording).\n",
        "- **Typical Range**: BERTScore ranges from 0 to 1. Scores above 0.85 generally signify strong semantic alignment with the reference, indicating high relevance and conceptual accuracy."
      ],
      "metadata": {
        "id": "bgYTwJqYxWmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)\n",
        "P, R, F1 = scorer.score([response_b], [response_a])\n",
        "bertscore_b = F1.item()\n",
        "P, R, F1 = scorer.score([response_c], [response_a])\n",
        "bertscore_c = F1.item()"
      ],
      "metadata": {
        "id": "cUaKHtqDxYkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"BERTScore for Response B: {bertscore_b:.3f}\")\n",
        "print(f\"BERTScore for Response C: {bertscore_c:.3f}\")"
      ],
      "metadata": {
        "id": "Th-hi94XxasO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ROUGE\n",
        "\n",
        "- **Interpretation**: A higher ROUGE score (typically, ROUGE-1, ROUGE-2, or ROUGE-L) suggests that the generated text covers more relevant phrases or sequences from the reference text. This indicates better **relevance** and **completeness**.\n",
        "- **Typical Range**: ROUGE scores are generally between 0 and 1 (or 0–100%). Scores closer to 1 mean greater similarity to the reference.\n",
        "  - ROUGE-1 shows overlap at the word level.\n",
        "  - ROUGE-2 is for bigram overlap.\n",
        "  - ROUGE-L captures overlap in sentence structureindicating coherence at the sentence level.\n",
        "  - Recall (r) measures how much of the reference text’s content is captured in the generated text.\n",
        "  - Precision (p) measures how relevant the generated text is to the reference text.\n",
        "  - F1-score (f) is a harmonic mean of recall and precision, balancing both to give a single representative score."
      ],
      "metadata": {
        "id": "JD0r0ctPxefg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b47f66e5-84ce-4aed-b967-1357654d24d3",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "p7_w52T-Amt_"
      },
      "outputs": [],
      "source": [
        "rouge = Rouge()\n",
        "scores_b = rouge.get_scores(response_b, response_a)[0]\n",
        "scores_c = rouge.get_scores(response_c, response_a)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2e98e7b6-516c-4c4d-985f-f443fbb3fd3d",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "ECZYsWFWmXXB"
      },
      "outputs": [],
      "source": [
        "print(\"\\nROUGE Score for Response B:\\n\\n\", pd.DataFrame(scores_b))\n",
        "print(\"\\nROUGE Score for Response C:\\n\\n\", pd.DataFrame(scores_c))"
      ]
    }
  ]
}