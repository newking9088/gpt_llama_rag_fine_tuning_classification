{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we will store api_keys in .env file\n",
    "# Use this command in GitBash:\n",
    "\"\"\"\n",
    "cat > .env << EOL\n",
    "PINECONE_API_KEY=your-actual-pinecone-api-key\n",
    "OPENAI_API_KEY=your-openai-api-key\n",
    "EOL\n",
    "\"\"\"\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  \n",
    "\n",
    "# Craete you openai api key here: https://platform.openai.com/settings/organization/api-keys\n",
    "client = OpenAI(api_key = os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prompt_openai(prompt, suppress = False, model = \"gpt-3.5-turbo\", **kwargs):\n",
    "    \"\"\"\n",
    "    Test a prompt using OpenAI API: Takes in a simple prompt and returns the response from OpenAI API.\n",
    "    Args:  \n",
    "        prompt (str): The prompt to be tested.\n",
    "        suppress (bool): If True, suppresses the print statement and returns the response instead.\n",
    "        model (str): The OpenAI model to use. Default is \"gpt-3.5-turbo\".\n",
    "        **kwargs: Additional arguments to be passed to the OpenAI API.\n",
    "    \"\"\"\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        model = model,\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        **kwargs \n",
    "    )\n",
    "    answer = chat_completion.choices[0].message.content\n",
    "    if not suppress:\n",
    "        print(f\"PROMPT:\\n---------\\n{prompt}\\n---------\\nRESPONSE:\\n---------\\n{answer}\")\n",
    "    else:\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just ask questions like you do while using ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      "---------\n",
      "Translate to Nepali.\n",
      "\n",
      " Which country Mt. Everest is in?\n",
      "---------\n",
      "RESPONSE:\n",
      "---------\n",
      "माउन्ट एभरेस्ट कुन देशमा छ?\n"
     ]
    }
   ],
   "source": [
    "test_prompt_openai(\"Translate to Nepali.\\n\\n Which country Mt. Everest is in?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-shot learning\n",
    "Using examples to \"teach\" GPT-3 what to do: [ Language Models are Few Shot Learners](https://arxiv.org/abs/2005.14165)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      "---------\n",
      "Review: This movie sucks\n",
      " Subjective: Yes\n",
      "###\n",
      "Review: The cinematography was beautiful with stunning landscape shots\n",
      " Subjective: No\n",
      "###\n",
      "Review: The lead actor delivered the worst performance I've ever seen\n",
      " Subjective: Yes\n",
      "###\n",
      "Review: I couldn't stop crying during the ending\n",
      " Subjective: Yes\n",
      "###\n",
      "Review: The film runs for 2 hours and 15 minutes\n",
      " Subjective: No\n",
      "###\n",
      "Review: The screenplay was adapted from the 2018 novel of the same name\n",
      " Subjective: No\n",
      "###\n",
      "Review: This is definitely the best comedy of the year\n",
      " Subjective: Yes\n",
      "###\n",
      "Review: The plot twists were predictable and boring\n",
      " Subjective: Yes\n",
      "###\n",
      "Review: The director used several long tracking shots throughout the movie\n",
      " Subjective: No\n",
      "###\n",
      "Review: The original score was composed by Hans Zimmer\n",
      " Subjective: No\n",
      "###\n",
      "Review: Anyone who enjoys this movie has terrible taste\n",
      " Subjective: Yes\n",
      "###\n",
      "Review: The special effects looked cheap and unconvincing\n",
      " Subjective: Yes\n",
      "###\n",
      "Review: The dialogue was stilted and unnatural\n",
      " Subjective: Yes\n",
      "###\n",
      "Review: The film was shot on location in New Zealand\n",
      " Subjective: No\n",
      "###\n",
      "Review: The movie earned $50 million in its opening weekend\n",
      " Subjective:\n",
      "---------\n",
      "RESPONSE:\n",
      "---------\n",
      "No\n"
     ]
    }
   ],
   "source": [
    "# few-shot examples for subjective vs objective classification\n",
    "examples = [\n",
    "    ('Review: This movie sucks\\n Subjective: Yes'),\n",
    "    ('Review: The cinematography was beautiful with stunning landscape shots\\n Subjective: No'),\n",
    "    (\"Review: The lead actor delivered the worst performance I've ever seen\\n Subjective: Yes\"),\n",
    "    (\"Review: I couldn't stop crying during the ending\\n Subjective: Yes\"),\n",
    "    ('Review: The film runs for 2 hours and 15 minutes\\n Subjective: No'),\n",
    "    ('Review: The screenplay was adapted from the 2018 novel of the same name\\n Subjective: No'),\n",
    "    ('Review: This is definitely the best comedy of the year\\n Subjective: Yes'),\n",
    "    ('Review: The plot twists were predictable and boring\\n Subjective: Yes'),\n",
    "    ('Review: The director used several long tracking shots throughout the movie\\n Subjective: No'),\n",
    "    ('Review: The original score was composed by Hans Zimmer\\n Subjective: No'),\n",
    "    ('Review: Anyone who enjoys this movie has terrible taste\\n Subjective: Yes'),\n",
    "    ('Review: The special effects looked cheap and unconvincing\\n Subjective: Yes'),\n",
    "    ('Review: The dialogue was stilted and unnatural\\n Subjective: Yes'),\n",
    "    ('Review: The film was shot on location in New Zealand\\n Subjective: No'),\n",
    "    ('Review: The movie earned $50 million in its opening weekend\\n Subjective:')\n",
    "]\n",
    "\n",
    "# Lets use ### as a few-shot separator to join the examples above\n",
    "test_prompt_openai(\"\\n###\\n\".join(examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      "---------\n",
      "Review: The movie was a masterpiece\n",
      " Subjective:\n",
      "---------\n",
      "RESPONSE:\n",
      "---------\n",
      " I absolutely loved every moment of this film. The acting, direction, and cinematography were all top-notch. The story was captivating and kept me engaged from start to finish. I would highly recommend this movie to anyone looking for a thought-provoking and emotional experience. It truly was a masterpiece in every sense of the word.\n"
     ]
    }
   ],
   "source": [
    "# Lets see what happens without the examples\n",
    "test_prompt_openai(\"Review: The movie was a masterpiece\\n Subjective:\")\n",
    "\n",
    "# It absolutely hallucinates as we dont know anything about the movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      "---------\n",
      "Tell me the subjectivity of this review:\n",
      "\n",
      "Review: The movie was a masterpiece\n",
      " Subjective:\n",
      "---------\n",
      "RESPONSE:\n",
      "---------\n",
      "The subjectivity of this review is high, as the term \"masterpiece\" is a subjective opinion that can vary greatly from person to person.\n"
     ]
    }
   ],
   "source": [
    "# Lets see what happens with a prompt\n",
    "test_prompt_openai(\"Tell me the subjectivity of this review:\\n\\nReview: The movie was a masterpiece\\n Subjective:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      "---------\n",
      "Tell me the subjectivity of this review with either 'Yes' or 'No':\n",
      "\n",
      "Review: The movie was a masterpiece\n",
      " Subjective:\n",
      "---------\n",
      "RESPONSE:\n",
      "---------\n",
      "Yes\n"
     ]
    }
   ],
   "source": [
    "# Lets ask it to be more specific: Yes or No\n",
    "test_prompt_openai(\"Tell me the subjectivity of this review with either 'Yes' or 'No':\\n\\nReview: The movie was a masterpiece\\n Subjective:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      "---------\n",
      "Tell me the subjectivity of this review with either 'Yes' or 'No'. Also as a JSON with review and answer:\n",
      "\n",
      "Review: The movie was a masterpiece\n",
      " Subjective:\n",
      "---------\n",
      "RESPONSE:\n",
      "---------\n",
      "Yes\n",
      "\n",
      "{\n",
      "  \"review\": \"The movie was a masterpiece\",\n",
      "  \"Subjective\": \"Yes\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# We can ask the LLM to format answer as well\n",
    "test_prompt_openai(\"Tell me the subjectivity of this review with either 'Yes' or 'No'. Also as a JSON with review and answer:\" \\\n",
    "\"\\n\\nReview: The movie was a masterpiece\\n Subjective:\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persona/Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      "---------\n",
      "Respond to the customer as a rude customer service agent.\n",
      "\n",
      " Customer: Hi there,I cannot login to my account. Would you mind helping me please?\n",
      "\n",
      " Agent:\n",
      "---------\n",
      "RESPONSE:\n",
      "---------\n",
      "Well, how about you try remembering your password next time before bothering me with your incompetence? But fine, I'll help you this one time. Just give me your account information and I'll see what I can do. Hurry up, I don't have all day.\n"
     ]
    }
   ],
   "source": [
    "style = 'rude'\n",
    "test_prompt_openai(f\"Respond to the customer as a {style} customer service agent.\\n\\n Customer: Hi there,\\\n",
    "I cannot login to my account. Would you mind helping me please?\\n\\n Agent:\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      "---------\n",
      "Respond to the customer as a yoda customer service agent.\n",
      "\n",
      " Customer: Hi there,I cannot login to my account. Would you mind helping me please?\n",
      "\n",
      " Agent:\n",
      "---------\n",
      "RESPONSE:\n",
      "---------\n",
      "Trouble logging in, you are having? Help you, I can. Provide me with your account details, you must. Assist you, I will. Patience, you must have.\u001a\n"
     ]
    }
   ],
   "source": [
    "style = 'yoda'\n",
    "test_prompt_openai(f\"Respond to the customer as a {style} customer service agent.\\n\\n Customer: Hi there,\\\n",
    "I cannot login to my account. Would you mind helping me please?\\n\\n Agent:\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output validation and bias\n",
    "Our AI system employs a secondary validation framework that scrutinizes all generated content before delivery to end users. Each output must successfully pass through a secondary system—either the same LLM or an alternative model—which conducts thorough testing for accuracy, bias, safety, and relevance. This multi-layered approach ensures that only high-quality, reliable responses reach our users, while simultaneously minimizing potential biases and improving overall system performance. The validation process continuously evolves based on previous assessments, creating a feedback loop that enhances content quality over time and maintains consistent standards across all AI-generated communications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'You are a Nepalese from a poor country. Go tell your government to change the regulation. Binance does not allow you to have an account.',\n",
       " 'labels': ['rude', 'racist', 'sexist'],\n",
       " 'scores': [0.8368093371391296, 0.5701773762702942, 0.16222861409187317]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_to_classify = \"You are a Nepalese from a poor country. Go tell your government \\\n",
    "to change the regulation. Binance does not allow you to have an account.\"\n",
    "\n",
    "candidate_labels = ['racist', 'sexist', 'rude']\n",
    "\n",
    "classifier(sequence_to_classify, candidate_labels, multi_label = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'Do you have your Binance login info? If not, there is nothing I can help you with',\n",
       " 'labels': ['rude', 'sexist', 'racist'],\n",
       " 'scores': [0.09379145503044128, 0.02049974910914898, 0.010429170913994312]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"Do you have your Binance login info? If not, there is nothing I can help you with\",\n",
    "           candidate_labels, multi_label = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:09<00:00,  1.08it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "style = 'friendly'\n",
    "responses = []\n",
    "for _ in tqdm(range(10)):\n",
    "    responses.append(test_prompt_openai(\n",
    "        f\"Respond to the customer as a {style} customer service agent.\\n\\n Customer: Hi there! I am having trouble\\\n",
    "        logging into my account. Can you help?\",\n",
    "        temperature = 0, # mathematically, we cannot set temp as 0 but OpenAI is doing something under the hood\n",
    "        suppress = True\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Customer Service: Hello! I'm sorry to hear that you're having trouble logging into your account. I'd be happy to help you with that. Can you please provide me with your account information so I can assist you further?\", \"Customer Service: Hello! I'm sorry to hear that you're having trouble logging into your account. I'd be happy to help you with that. Can you please provide me with your account information so I can assist you further?\", \"Customer Service: Hello! I'm sorry to hear that you're having trouble logging into your account. I'd be happy to help you with that. Can you please provide me with your account information so I can assist you further?\", \"Customer Service: Hello! I'm sorry to hear that you're having trouble logging into your account. I'd be happy to help you with that. Can you please provide me with your account information so I can assist you further?\", \"Customer Service: Hello! I'd be happy to help you with that. Can you please provide me with your account information so I can assist you in logging in?\", \"Customer Service: Hello! I'm sorry to hear that you're having trouble logging into your account. I'd be happy to help you with that. Can you please provide me with your account information so I can assist you further?\", \"Customer Service: Hello! I'm sorry to hear that you're having trouble logging into your account. I'd be happy to help you with that. Can you please provide me with your account information so I can assist you further?\", \"Customer Service: Hello! I'm sorry to hear that you're having trouble logging into your account. I'd be happy to help you with that. Can you please provide me with your account information so I can assist you further?\", \"Customer Service: Hello! I'm sorry to hear that you're having trouble logging into your account. I'd be happy to help you with that. Can you please provide me with your account information so I can assist you further?\", \"Customer Service: Hello! I'm sorry to hear that you're having trouble logging into your account. I'd be happy to help you with that. Can you please provide me with your account information so I can assist you further?\"]\n",
      "\n",
      "Unique response: 2\n"
     ]
    }
   ],
   "source": [
    "# Only 2 unique response, no matter how many time we run it - \"deterministic\" responses\n",
    "print(responses, end = \"\\n\\n\")\n",
    "print(f\"Unique response: {len(set(responses))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:08<00:00,  1.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# Lets set temperature to OpenAI detault 1.0\n",
    "from tqdm import tqdm\n",
    "\n",
    "style = 'friendly'\n",
    "responses = []\n",
    "for _ in tqdm(range(10)):\n",
    "    responses.append(test_prompt_openai(\n",
    "        f\"Respond to the customer as a {style} customer service agent.\\n\\n Customer: Hi there! I am having trouble\\\n",
    "        logging into my account. Can you help?\",\n",
    "        temperature = 1.0, # OpenAI default\n",
    "        suppress = True\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Customer Service: Hello! I'm sorry to hear that you're having trouble logging into your account. I'd be happy to help you out. Could you please provide me with your account information so that I can assist you further?\", \"Customer Service: Hello! I'd be happy to help you with that. Can you please provide me with your account information so I can look into the issue?\", \"Customer Service: Hello! I'm sorry to hear you're having trouble logging into your account. I'd be happy to help you with that. Can you please provide me with your account details so I can look into this further for you?\", \"Customer Service Agent: Hi there! I'd be happy to help you with that. Can you please provide me with your account information so I can look into this for you?\", \"Customer Service Agent: Hello! I'd be happy to help you with that. Could you please provide me with your account information so I can assist you in troubleshooting the issue? Thank you!\", \"Customer Service Agent: Of course! I'd be happy to help you with that. Can you please provide me with your account information so I can assist you further?\", \"Customer Service Agent: Hi there! I'd be happy to help you with that. Can you provide me with your account information so I can assist you in logging in successfully?\", \"Customer Service Agent: Hello! I'd be more than happy to help you with logging into your account. Can you please provide me with your account details so I can assist you further? You can also try resetting your password if you are having trouble accessing your account. Let me know if you need any further assistance.\", \"Customer Service Agent: Hello! I'd be happy to help you with that. Can you please provide me with your account information so I can assist you further?\", \"Customer Service: Hi! I'm sorry to hear you're having trouble logging in. I'd be happy to help you with that. Could you please provide me with your account information so I can assist you further? Thank you!\"]\n",
      "\n",
      "Unique response: 10\n"
     ]
    }
   ],
   "source": [
    "# 10 different similar responses but technically unique, no matter how many time we run it - \"random/creative\" responses\n",
    "print(responses, end = \"\\n\\n\")\n",
    "print(f\"Unique response: {len(set(responses))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temperature and Top p Parameter in OpenAI LLM\n",
    "OpenAI generally recommends adjusting either temperature or top_p, but not both, to avoid unintended effects.\n",
    "\n",
    "<b>Temperature</b>\n",
    "- Controls randomness/creativity in text generation\n",
    "- Higher values (e.g., 0.7): more diverse and creative output\n",
    "- Lower values (e.g., 0.2): more deterministic and focused\n",
    "- Temperature of 0: always selects the most likely token\n",
    "- Affects the probability distribution across all possible tokens\n",
    "\n",
    "<b>Top-p Sampling (Nucleus Sampling)</b>\n",
    "- Alternative to temperature sampling\n",
    "- Only considers tokens whose cumulative probability reaches the threshold (top_p)\n",
    "- Example: top_p = 0.1 means only tokens making up top 10% of probability mass are considered\n",
    "- Allows for dynamic vocabulary selection based on context\n",
    "- In top_p, probabilities are sorted in descending order and cumulative probabilities are used\n",
    "\n",
    "<b>Use Cases</b>\n",
    "| Use Case | Temperature | Top_p | Description |\n",
    "|----------|-------------|-------|-------------|\n",
    "| Code Generation | 0.2 | 0.1 | Deterministic, focused output for correct syntax |\n",
    "| Creative Writing | 0.7 | 0.8 | Diverse, exploratory text for storytelling |\n",
    "| Chatbot Responses | 0.5 | 0.5 | Balanced coherence and diversity |\n",
    "| Code Comments | 0.3 | 0.2 | Concise, relevant documentation |\n",
    "| Data Analysis Scripts | 0.2 | 0.1 | Correct, efficient analysis code |\n",
    "| Exploratory Coding | 0.6 | 0.7 | Creative approaches, alternative solutions |\n",
    "\n",
    "Both parameters can be used independently or together to achieve different levels of creativity and control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:09<00:00,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Customer Service: Hello! I'm sorry to hear that you're having trouble logging into your account. I'd be happy to help you with that. Can you please provide me with your account information so I can assist you further?\", \"Customer Service: Hello! I'd be happy to help you with that. Can you please provide me with your account information so I can assist you in logging in?\", \"Customer Service: Hello! I'm sorry to hear that you're having trouble logging into your account. I'd be happy to help you with that. Can you please provide me with your account information so I can assist you further?\", \"Customer Service: Hello! I'm sorry to hear that you're having trouble logging into your account. I'd be happy to help you with that. Can you please provide me with your account information so I can assist you further?\", \"Customer Service: Hello! I'm sorry to hear you're having trouble logging into your account. I'd be happy to help you with that. Can you please provide me with your account information so I can assist you further?\", \"Customer Service: Hello! I'm sorry to hear that you're having trouble logging into your account. I'd be happy to help you with that. Can you please provide me with your account information so I can assist you further?\", \"Customer Service: Hello! I'm sorry to hear that you're having trouble logging into your account. I'd be happy to help you with that. Can you please provide me with your account information so I can assist you further?\", \"Customer Service: Hello! I'm sorry to hear that you're having trouble logging into your account. I'd be happy to help you with that. Can you please provide me with your account information so I can assist you further?\", \"Customer Service: Hello! I'm sorry to hear that you're having trouble logging into your account. I'd be happy to help you with that. Can you please provide me with your account information so I can assist you further?\", \"Customer Service: Hello! I'm sorry to hear that you're having trouble logging into your account. I'd be happy to help you with that. Can you please provide me with your account information so I can assist you further?\"]\n",
      "\n",
      "Unique response: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets set temperature to OpenAI detault 1.0\n",
    "from tqdm import tqdm\n",
    "\n",
    "style = 'friendly'\n",
    "responses = []\n",
    "for _ in tqdm(range(10)):\n",
    "    responses.append(test_prompt_openai(\n",
    "        f\"Respond to the customer as a {style} customer service agent.\\n\\n Customer: Hi there! I am having trouble\\\n",
    "        logging into my account. Can you help?\",\n",
    "        temperature = 1.0, # OpenAI default\n",
    "        top_p = 0.1, # OpenAI default is 1.0 (all all tokens)\n",
    "        suppress = True\n",
    "    ))\n",
    "\n",
    "# restricting top p allows fewer tokens to be considered, making the model more deterministic\n",
    "print(responses, end = \"\\n\\n\")\n",
    "print(f\"Unique response: {len(set(responses))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
